{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from collections import Counter\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataPipeline(Dataset):\n",
    "    def __init__(self, filename,window_size = 4,min_freq=1,vocab=None,neg_words=5):\n",
    "        self.data = self.read_data(filename)\n",
    "        self.neg_words = neg_words\n",
    "        self.window_size = window_size\n",
    "        if vocab is None:\n",
    "            self.vocab, self.ind2vocab,self.word_count = self.build_vocab(self.data)\n",
    "        else:\n",
    "            self.vocab = vocab\n",
    "            self.ind2vocab = {v: k for k, v in vocab.items()}\n",
    "            self.word_count = self.get_word_count(vocab,self.data)\n",
    "        self.neg_sampling_table = self.__create_neg_sampling_table()\n",
    "        self.sub_sampling_table = self.__create_sub_sampling_table()\n",
    "\n",
    "    def get_vocab(self):\n",
    "        return self.vocab\n",
    "\n",
    "    def read_data(self, filename):\n",
    "        data = []\n",
    "        with open(filename, 'r') as f:\n",
    "            for line in f.readlines():\n",
    "                e = line.strip()\n",
    "                data.append(e.split())\n",
    "        return data\n",
    "    \n",
    "    def get_word_count(self,vocab,data):\n",
    "        word_count = {0: 0}\n",
    "        for line in data:\n",
    "            for word in line:\n",
    "                if word in vocab:\n",
    "                    word_count[vocab[word]] += 1\n",
    "                else:\n",
    "                    word_count[0] += 1\n",
    "        return word_count\n",
    "    \n",
    "    def most_common(self,n):\n",
    "        counter = Counter(self.word_count)\n",
    "        common = counter.most_common(n)\n",
    "        ind_freq = dict(common)\n",
    "        # convert to word frequency\n",
    "        word_freq = {}\n",
    "        for ind in ind_freq:\n",
    "            word_freq[self.ind2vocab[ind]] = ind_freq[ind]\n",
    "        return word_freq\n",
    "\n",
    "    def build_vocab(self, data,min_freq=1):\n",
    "        word_set = {}\n",
    "        for line in data:\n",
    "            for word in line:\n",
    "                if word not in word_set:\n",
    "                    word_set[word]=1\n",
    "                else:\n",
    "                    word_set[word]+=1\n",
    "        # sort the vocab\n",
    "        word_list = sorted(list(word_set))\n",
    "        word_count = {0: 1}\n",
    "        vocab_dict = {\"<unk>\": 0}\n",
    "        i=1\n",
    "        for word in word_list:\n",
    "            if word_set[word] >= min_freq:\n",
    "                vocab_dict[word] = i\n",
    "                word_count[i] = word_set[word]\n",
    "                i+=1\n",
    "            else:\n",
    "                word_count[0] += word_set[word]\n",
    "        ind2word = {v: k for k, v in vocab_dict.items()}\n",
    "        return vocab_dict, ind2word, word_count\n",
    "\n",
    "    def total_count(self):\n",
    "        return sum(self.word_count.values())\n",
    "\n",
    "    \n",
    "    def __create_sub_sampling_table(self, threshold=1e-5):\n",
    "        word_freq = np.array(list(self.word_count.values()))\n",
    "        word_freq = word_freq / np.sum(word_freq)\n",
    "        sub_sampling_table = ((np.sqrt(word_freq / threshold) + 1) * (threshold / word_freq))\n",
    "        return sub_sampling_table\n",
    "    \n",
    "    def is_sample_selected(self, idx):\n",
    "        # return True if the word is selected\n",
    "        return random.random() < self.sub_sampling_table[idx]\n",
    "    \n",
    "    def __create_neg_sampling_table(self, power=0.75, table_size =1e8):\n",
    "        vocab_size = len(self.vocab)\n",
    "        word_freq = np.array(list(self.word_count.values())) ** power\n",
    "        word_freq = word_freq / np.sum(word_freq)\n",
    "        count = np.round(word_freq * table_size)\n",
    "        neg_sampling_table = []\n",
    "        for i in range(vocab_size):\n",
    "            neg_sampling_table += [i] * int(count[i])\n",
    "        neg_sampling_table = np.array(neg_sampling_table)\n",
    "        np.random.shuffle(neg_sampling_table)\n",
    "        return neg_sampling_table.tolist()\n",
    "    \n",
    "    def get_negative_samples(self, target, k):\n",
    "        delta = random.sample(self.neg_sampling_table, k)\n",
    "        while target in delta:\n",
    "            delta = random.sample(self.neg_sampling_table, k)\n",
    "        return delta   \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        words = self.data[idx]\n",
    "        if len(words) < self.window_size:\n",
    "            raise Exception(\"Sentence length is less than window size\")\n",
    "        data = []\n",
    "        start = self.window_size // 2\n",
    "        for i in range(start, len(words) - start):\n",
    "            target = self.vocab[words[i]]\n",
    "            if not self.is_sample_selected(target):\n",
    "                continue\n",
    "            context = words[i - start: i] + words[i + 1: i + start + 1]\n",
    "            context = [self.vocab[word] for word in context]\n",
    "            neg_samples = self.get_negative_samples(target, self.neg_words)\n",
    "            data.append((target, context, neg_samples))\n",
    "        return data\n",
    "    \n",
    "    def __collate_fn(self,batches):\n",
    "        target = []\n",
    "        context = []\n",
    "        neg_samples = []\n",
    "        for sentence in batches:\n",
    "            for t,c,n in sentence:\n",
    "                target.append(t)\n",
    "                context.append(c)\n",
    "                neg_samples.append(n)\n",
    "        return torch.LongTensor(target),torch.LongTensor(context),torch.LongTensor(neg_samples)\n",
    "\n",
    "    def get_batches(self, batch_size):\n",
    "        return DataLoader(self, batch_size=batch_size, shuffle=False,collate_fn=self.__collate_fn ,drop_last=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = DataPipeline(\"../data/corpus_cleaned.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "742"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.word_count[dataset.vocab['k']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'the': 568205,\n",
       " 'and': 298569,\n",
       " 'a': 268778,\n",
       " 'of': 257422,\n",
       " 'to': 225717,\n",
       " 'is': 203966,\n",
       " 'in': 155513,\n",
       " 'this': 150864,\n",
       " 'it': 150259,\n",
       " 'i': 146230,\n",
       " 'that': 113076,\n",
       " 'movie': 94233,\n",
       " 'for': 82555,\n",
       " 'as': 79279,\n",
       " 'was': 78510,\n",
       " 'with': 75268,\n",
       " 'but': 65319,\n",
       " 'you': 64244,\n",
       " 'film': 60895,\n",
       " 'on': 60665}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100008668"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset.neg_sampling_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.word_count[dataset.vocab[\"noodles\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.020088256634534823"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.sub_sampling_table[dataset.vocab[\"of\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Negative samples = freq :\n",
      "get = 15939\n",
      "plays = 4213\n",
      "actually = 5119\n",
      "book = 6687\n",
      "not = 52474\n",
      "master = 1127\n"
     ]
    }
   ],
   "source": [
    "words=[ dataset.ind2vocab[idx] for idx in dataset.get_negative_samples(dataset.vocab[\"the\"], 6)]\n",
    "print('Negative samples = freq :')\n",
    "for word in words :\n",
    "    print('{} = {}'.format(word,dataset.word_count[dataset.vocab[word]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = 5\n",
    "words = \"a b c d e f g h i j k l m n o p q r s t u v w x y z\".split()\n",
    "start = window_size // 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target: c \t Context: ['a', 'b', 'd', 'e']\n",
      "Target: d \t Context: ['b', 'c', 'e', 'f']\n",
      "Target: e \t Context: ['c', 'd', 'f', 'g']\n",
      "Target: f \t Context: ['d', 'e', 'g', 'h']\n",
      "Target: g \t Context: ['e', 'f', 'h', 'i']\n",
      "Target: h \t Context: ['f', 'g', 'i', 'j']\n",
      "Target: i \t Context: ['g', 'h', 'j', 'k']\n",
      "Target: j \t Context: ['h', 'i', 'k', 'l']\n",
      "Target: k \t Context: ['i', 'j', 'l', 'm']\n",
      "Target: l \t Context: ['j', 'k', 'm', 'n']\n",
      "Target: m \t Context: ['k', 'l', 'n', 'o']\n",
      "Target: n \t Context: ['l', 'm', 'o', 'p']\n",
      "Target: o \t Context: ['m', 'n', 'p', 'q']\n",
      "Target: p \t Context: ['n', 'o', 'q', 'r']\n",
      "Target: q \t Context: ['o', 'p', 'r', 's']\n",
      "Target: r \t Context: ['p', 'q', 's', 't']\n",
      "Target: s \t Context: ['q', 'r', 't', 'u']\n",
      "Target: t \t Context: ['r', 's', 'u', 'v']\n",
      "Target: u \t Context: ['s', 't', 'v', 'w']\n",
      "Target: v \t Context: ['t', 'u', 'w', 'x']\n",
      "Target: w \t Context: ['u', 'v', 'x', 'y']\n",
      "Target: x \t Context: ['v', 'w', 'y', 'z']\n"
     ]
    }
   ],
   "source": [
    "for i in range(start, len(words) - start):\n",
    "    context = words[i - start: i] + words[i + 1: i + start + 1]\n",
    "    target = words[i]\n",
    "    print('Target: {} \\t Context: {}'.format(target, context))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target: tips \t Context: ['some', 'great', 'as', 'always'] \t Negative Samples: ['others', 'fun', 'life', 'tableaux', 'album']\n",
      "Target: helping \t Context: ['and', 'is', 'me', 'to'] \t Negative Samples: ['bruce', 'insert', 'or', 'effective', 'day']\n",
      "Target: eats \t Context: ['my', 'good', 'collection', 'i'] \t Negative Samples: ['but', 'are', 'gorgeousness', 'take', 'least']\n",
      "Target: collection \t Context: ['good', 'eats', 'i', 'havent'] \t Negative Samples: ['for', 'im', 'will', 'of', 'drunk']\n",
      "Target: any \t Context: ['havent', 'tried', 'of', 'the'] \t Negative Samples: ['kimer', 'i', 'fi', 'myself', 'sad']\n",
      "Target: recipes \t Context: ['of', 'the', 'yet', 'but'] \t Negative Samples: ['portrayed', 'recalls', 'cd', 'and', 'bissetdirector']\n",
      "Target: i \t Context: ['yet', 'but', 'will', 'soon'] \t Negative Samples: ['base', 'new', 'the', 'scene', 'ie']\n",
      "Target: lovely \t Context: ['its', 'just', 'to', 'let'] \t Negative Samples: ['this', 'hasnt', 'fiction', 'on', 'italthough']\n",
      "Target: alton \t Context: ['to', 'let', 'entertain', 'us'] \t Negative Samples: ['narcisstic', 'isnt', 'specialsthere', 'war', 'rauffenstein']\n"
     ]
    }
   ],
   "source": [
    "sample0=dataset.__getitem__(0)\n",
    "for var in sample0:\n",
    "    t,c,n = var\n",
    "    t=dataset.ind2vocab[t]\n",
    "    c=[dataset.ind2vocab[idx] for idx in c]\n",
    "    n=[dataset.ind2vocab[idx] for idx in n]\n",
    "    print('Target: {} \\t Context: {} \\t Negative Samples: {}'.format(t,c,n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "batches = dataset.get_batches(128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2088]) torch.Size([2088, 4]) torch.Size([2088, 5])\n"
     ]
    }
   ],
   "source": [
    "for batch in batches:\n",
    "    x = 0\n",
    "    target,context,neg_samples = batch\n",
    "    print(target.shape,context.shape,neg_samples.shape)\n",
    "    break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
