{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from collections import Counter\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import random\n",
    "class DataPipeline(Dataset):\n",
    "    def __init__(self, filename,window_size = 7,min_freq=5,vocab=None,neg_words=5):\n",
    "        self.data = self.read_data(filename)\n",
    "        self.neg_words = neg_words\n",
    "        self.window_size = window_size\n",
    "        if vocab is None:\n",
    "            self.vocab, self.ind2vocab,self.word_count = self.build_vocab(self.data,min_freq)\n",
    "        else:\n",
    "            self.vocab = vocab\n",
    "            self.ind2vocab = {v: k for k, v in vocab.items()}\n",
    "            self.word_count = self.get_word_count(vocab,self.data,min_freq)\n",
    "        self.neg_sampling_table = self.__create_neg_sampling_table()\n",
    "        self.sub_sampling_table = self.__create_sub_sampling_table()\n",
    "\n",
    "    def get_vocab(self):\n",
    "        return self.vocab\n",
    "    \n",
    "    @staticmethod\n",
    "    def read_data(filename):\n",
    "        data = []\n",
    "        with open(filename, 'r') as f:\n",
    "            for line in f.readlines():\n",
    "                e = line.strip()\n",
    "                data.append(e.split())\n",
    "        return data\n",
    "    \n",
    "    def get_word_count(self,vocab,data):\n",
    "        word_count = {0: 0}\n",
    "        for line in data:\n",
    "            for word in line:\n",
    "                if word in vocab:\n",
    "                    word_count[vocab[word]] += 1\n",
    "                else:\n",
    "                    word_count[0] += 1\n",
    "        return word_count\n",
    "    \n",
    "    def most_common(self,n):\n",
    "        counter = Counter(self.word_count)\n",
    "        common = counter.most_common(n)\n",
    "        ind_freq = dict(common)\n",
    "        # convert to word frequency\n",
    "        word_freq = {}\n",
    "        for ind in ind_freq:\n",
    "            word_freq[self.ind2vocab[ind]] = ind_freq[ind]\n",
    "        return word_freq\n",
    "    \n",
    "    @staticmethod\n",
    "    def build_vocab(data,min_freq=10):\n",
    "        word_set = {}\n",
    "        for line in data:\n",
    "            for word in line:\n",
    "                if word not in word_set:\n",
    "                    word_set[word]=1\n",
    "                else:\n",
    "                    word_set[word]+=1\n",
    "        # sort the vocab\n",
    "        word_list = sorted(list(word_set))\n",
    "        word_count = {0: 1}\n",
    "        vocab_dict = {\"<unk>\": 0}\n",
    "        i=1\n",
    "        for word in word_list:\n",
    "            if word_set[word] >= min_freq:\n",
    "                vocab_dict[word] = i\n",
    "                word_count[i] = word_set[word]\n",
    "                i+=1\n",
    "            else:\n",
    "                word_count[0] += word_set[word]\n",
    "        ind2word = {v: k for k, v in vocab_dict.items()}\n",
    "        return vocab_dict, ind2word, word_count\n",
    "\n",
    "    def total_count(self):\n",
    "        return sum(self.word_count.values())\n",
    "\n",
    "    \n",
    "    def __create_sub_sampling_table(self, threshold=1e-5):\n",
    "        word_freq = np.array(list(self.word_count.values()))\n",
    "        word_freq = word_freq / np.sum(word_freq)\n",
    "        sub_sampling_table = ((np.sqrt(word_freq / threshold) + 1) * (threshold / word_freq))\n",
    "        return sub_sampling_table\n",
    "    \n",
    "    def is_sample_selected(self, idx):\n",
    "        # return True if the word is selected\n",
    "        return random.random() < self.sub_sampling_table[idx]\n",
    "    \n",
    "    def __create_neg_sampling_table(self, power=0.75, table_size =1e8):\n",
    "        vocab_size = len(self.vocab)\n",
    "        word_freq = np.array(list(self.word_count.values())) ** power\n",
    "        word_freq = word_freq / np.sum(word_freq)\n",
    "        count = np.round(word_freq * table_size)\n",
    "        neg_sampling_table = []\n",
    "        for i in range(vocab_size):\n",
    "            neg_sampling_table += [i] * int(count[i])\n",
    "        neg_sampling_table = np.array(neg_sampling_table)\n",
    "        np.random.shuffle(neg_sampling_table)\n",
    "        return neg_sampling_table.tolist()\n",
    "    \n",
    "    def get_negative_samples(self, target, k):\n",
    "        delta = random.sample(self.neg_sampling_table, k)\n",
    "        while target in delta:\n",
    "            delta = random.sample(self.neg_sampling_table, k)\n",
    "        return delta   \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        words = self.data[idx]\n",
    "        data = []\n",
    "        start = self.window_size // 2\n",
    "        for i in range(start, len(words) - start):\n",
    "            if words[i] not in self.vocab or not self.is_sample_selected(self.vocab[words[i]]):\n",
    "                continue\n",
    "            target = self.vocab[words[i]]\n",
    "            context = words[i - start: i] + words[i + 1: i + start + 1]\n",
    "            #convert words to indices and unknown words to 0\n",
    "            context = [ self.vocab[word] if word in self.vocab else self.vocab[\"<unk>\"] for word in context ]\n",
    "            neg_samples = self.get_negative_samples(target, self.neg_words)\n",
    "            data.append((target, context, neg_samples))\n",
    "        return data\n",
    "    \n",
    "    def __collate_fn(self,batches):\n",
    "        target = []\n",
    "        context = []\n",
    "        neg_samples = []\n",
    "        for sentence in batches:\n",
    "            for t,c,n in sentence:\n",
    "                target.append(t)\n",
    "                context.append(c)\n",
    "                neg_samples.append(n)\n",
    "        return torch.LongTensor(target),torch.LongTensor(context),torch.LongTensor(neg_samples)\n",
    "\n",
    "    def get_batches(self, batch_size):\n",
    "        return DataLoader(self, batch_size=batch_size, shuffle=False,collate_fn=self.__collate_fn ,drop_last=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = DataPipeline.read_data('../data/processed_data/corpus_cleaned.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab, ind2vocab, word_count = DataPipeline.build_vocab(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23522"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import csr_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SVD_W2V:\n",
    "    def __init__(self, vocab, window,embedding_size):\n",
    "        self.dim = len(vocab)\n",
    "        self.vocab = vocab\n",
    "        self.window_size = window\n",
    "        self.embedding_size = embedding_size\n",
    "        self.cooccurrence_matrix = csr_matrix((self.dim, self.dim), dtype=np.int32)\n",
    "\n",
    "    def train(self,data):\n",
    "        self.__build_cooccurrence_matrix(data)\n",
    "\n",
    "\n",
    "    def __build_cooccurrence_matrix(self, data):\n",
    "        for tokens in data:\n",
    "            for pos,token in enumerate(tokens):\n",
    "                if token not in self.vocab:\n",
    "                    continue\n",
    "                start = max(0, pos - self.window_size)\n",
    "                end = min(len(tokens), pos + self.window_size)\n",
    "                for context_pos in range(start, end):\n",
    "                    if context_pos != pos:\n",
    "                        context_token = tokens[context_pos]\n",
    "                        if context_token in self.vocab:\n",
    "                            self.cooccurrence_matrix[self.vocab[token], self.vocab[context_token]] += 1\n",
    "                        else:\n",
    "                            self.cooccurrence_matrix[self.vocab[token], self.vocab[\"<unk>\"]] += 1\n",
    "    \n",
    "    def save_embeddings(self, path):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SVD_W2V(vocab, 7,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(23522, 23522)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.cooccurrence_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/soumodiptab/.local/lib/python3.8/site-packages/scipy/sparse/_index.py:82: SparseEfficiencyWarning: Changing the sparsity structure of a csr_matrix is expensive. lil_matrix is more efficient.\n",
      "  self._set_intXint(row, col, x.flat[0])\n"
     ]
    }
   ],
   "source": [
    "model.train(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
