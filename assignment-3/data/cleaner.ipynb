{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assignment - 3  NLP\n",
    "def create_sample_dataset(len,input_file,output_file):\n",
    "    with open(input_file, 'r') as f:\n",
    "        with open(output_file, 'w') as f1:\n",
    "            for _ in range(len):\n",
    "                f1.write(f.readline())\n",
    "\n",
    "create_sample_dataset(90000,'./reviews_Movies_and_TV.json','./sampled_data/movie_reviews.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "df = pd.read_json('./sampled_data/movie_reviews.json',lines=True)\n",
    "#f = json.loads(open('./sampled_data/movie_reviews.json','r').read())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"This has some great tips as always and is helping me to complete my Good Eats collection.  I haven't tried any of the recipes yet, but I will soon.  Sometimes it's just lovely to let Alton entertain us.\""
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_to_text_file(file):\n",
    "    with open(file,'w') as f:\n",
    "        # save the column reviewText to a text file\n",
    "        for line in df['reviewText'].to_list():\n",
    "            f.write(line+\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_to_text_file('./sampled_data/corpus.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchtext\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import re\n",
    "from cleantext import clean\n",
    "import os\n",
    "import random\n",
    "import math\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the data \n",
    "tokenizer = get_tokenizer('basic_english')\n",
    "\n",
    "def replace_dates(text):\n",
    "        date_format_a = re.sub(r'\\d{1,2}/\\d{1,2}/\\d{2,4}', ' <DATE> ', text)\n",
    "        date_format_b = re.sub(\n",
    "            r'[A-Za-z]{2,8}\\s\\d{1,2},?\\s\\d {4}', ' <DATE> ', date_format_a)\n",
    "        date_format_c = re.sub(\n",
    "            r'\\d{2} [A-Z][a-z]{2,8} \\d{4}', ' <DATE> ', date_format_b)\n",
    "        return date_format_c\n",
    "\n",
    "def replace_hash_tags(text):\n",
    "        return re.sub(r'(\\s|^)#(\\w+)', ' <HASHTAG> ', text)\n",
    "\n",
    "def remove_special_characters(text):\n",
    "        # remove all special characters \n",
    "        return re.sub(r'[^A-Za-z0-9\\s]', ' ', text)\n",
    "\n",
    "def remove_extra_spaces(text):\n",
    "        return re.sub(r'\\s{2,}', ' ', text)\n",
    "\n",
    "def replace_hyphenated_words(text):\n",
    "        # replace hyphenated words with words seperated by space\n",
    "        return re.sub(r'(\\w+)-(\\w+)', r' \\1 \\2 ', text)\n",
    "\n",
    "def read_data(filename,line_count,atleast=10):\n",
    "    with open(filename, 'r') as f:\n",
    "        lines = []\n",
    "        ctr = 0\n",
    "        for line in f.readlines():\n",
    "            line = line.strip()\n",
    "            line = re.sub(r'<|>', ' ', line)\n",
    "            line = replace_dates(line)\n",
    "            line = replace_hyphenated_words(line)\n",
    "            line = replace_hash_tags(line)\n",
    "            # remove < and > from the text\n",
    "            line = clean(line, no_emoji=True,\n",
    "                         no_urls=True,\n",
    "                         no_emails=True,\n",
    "                         no_phone_numbers=True,\n",
    "                         no_currency_symbols=True,           \n",
    "                         replace_with_url=\" <URL> \",\n",
    "                         replace_with_email=\" <EMAIL> \",\n",
    "                         replace_with_phone_number=\" <PHONE> \",\n",
    "                         replace_with_currency_symbol=\" <CURRENCY> \",\n",
    "                         lower=True)\n",
    "            line = remove_special_characters(line)\n",
    "            line = clean(line,no_numbers=True,no_digits=True,no_punct=True, replace_with_number=\" <NUMBER> \",replace_with_digit=\" \",replace_with_punct=\"\")\n",
    "            line = remove_extra_spaces(line)\n",
    "            tokens=tokenizer(line)\n",
    "            if len(tokens)>atleast:\n",
    "                lines.append(tokens)\n",
    "                ctr+=1\n",
    "            if ctr >= line_count:\n",
    "                break\n",
    "    return lines\n",
    "\n",
    "\n",
    "def save_data(filename, lines):\n",
    "    # Save the data to a file\n",
    "    with open(filename, 'w')as f:\n",
    "        for line in lines:\n",
    "            line = ' '.join(line)\n",
    "            f.write(line.strip()+'\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = read_data('./sampled_data/corpus.txt',80000,atleast=10)\n",
    "save_data('./processed_data/corpus_cleaned.txt', lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vocab(file_path):\n",
    "    vocab = set()\n",
    "    with open(file_path, 'r') as f:\n",
    "        for line in f.readlines():\n",
    "            line = line.strip()\n",
    "            tokens = line.split()\n",
    "            vocab.update(tokens)\n",
    "    vocab=sorted(vocab)\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = get_vocab('./processed_data/corpus_cleaned.txt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "json.dump(vocab,open('./processed_data/vocab.json','w'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
