{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: conllu in /home/soumodiptab/.local/lib/python3.8/site-packages (4.5.2)\n"
     ]
    }
   ],
   "source": [
    "# Install dependencies\n",
    "!pip install conllu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import headers\n",
    "from conllu import parse,parse_incr\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence, pad_packed_sequence, pack_padded_sequence\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TokenList<what, is, the, cost, of, a, round, trip, flight, from, pittsburgh, to, atlanta, beginning, on, april, twenty, fifth, and, returning, on, may, sixth, metadata={sent_id: \"0001.train\", text: \"what is the cost of a round trip flight from pittsburgh to atlanta beginning on april twenty fifth and returning on may sixth\"}>\n"
     ]
    }
   ],
   "source": [
    "with open (\"data/UD_English-Atis/en_atis-ud-train.conllu\", \"r\", encoding=\"utf-8\") as f:\n",
    "    data = f.read()\n",
    "train_sentences = parse(data)\n",
    "with open (\"data/UD_English-Atis/en_atis-ud-test.conllu\", \"r\", encoding=\"utf-8\") as f:\n",
    "    data = f.read()\n",
    "dev_sentences = parse(data)\n",
    "with open (\"data/UD_English-Atis/en_atis-ud-test.conllu\", \"r\", encoding=\"utf-8\") as f:\n",
    "    data = f.read()\n",
    "test_sentences = parse(data)\n",
    "print(train_sentences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 2,\n",
       " 'form': 'is',\n",
       " 'lemma': 'be',\n",
       " 'upos': 'AUX',\n",
       " 'xpos': None,\n",
       " 'feats': {'Mood': 'Ind',\n",
       "  'Number': 'Sing',\n",
       "  'Person': '3',\n",
       "  'Tense': 'Pres',\n",
       "  'VerbForm': 'Fin'},\n",
       " 'head': 1,\n",
       " 'deprel': 'cop',\n",
       " 'deps': None,\n",
       " 'misc': None}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = train_sentences[0]\n",
    "sentence[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_embeddings(filename, vocab_size=10000):\n",
    "    with open(filename, encoding=\"utf-8\") as file:\n",
    "        word_embedding_dim = len(file.readline().split(\" \")) - 1\n",
    "    vocab = {}\n",
    "    embeddings = np.zeros((vocab_size, word_embedding_dim))\n",
    "    with open(filename, encoding=\"utf-8\") as file:\n",
    "        for idx, line in enumerate(file):\n",
    "            if idx + 2 >= vocab_size:\n",
    "                break\n",
    "            cols = line.rstrip().split(\" \")\n",
    "            val = np.array(cols[1:])\n",
    "            word = cols[0]\n",
    "            embeddings[idx + 2] = val\n",
    "            vocab[word] = idx + 2\n",
    "    vocab[\"<UNK>\"]=1\n",
    "    vocab[\"<PAD>\"]=0\n",
    "  # a FloatTensor is a multidimensional matrix\n",
    "  # that contains 32-bit floats in every entry\n",
    "  # https://pytorch.org/docs/stable/tensors.html\n",
    "    return torch.FloatTensor(embeddings), vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 50000\n",
    "embeddings, vocab = read_embeddings('./data/glove.6B.50d.txt', vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence[0][\"form\"].lower() in vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocab(sentences):\n",
    "    data =[]\n",
    "    word_set = set()\n",
    "    vocab_dict={\"<PAD>\":0,\"<UNK>\":1}\n",
    "    for sent in sentences:\n",
    "        for token in sent:\n",
    "            word_set.add(token[\"form\"])\n",
    "    word_list = sorted(list(word_set))\n",
    "    for i,word in enumerate(word_list):\n",
    "        vocab_dict[word]=i+2\n",
    "    return vocab_dict\n",
    "#vocab_dict=build_vocab(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tags(sentences):\n",
    "    tag_set=set()\n",
    "    for sent in sentences:\n",
    "        for token in sent:\n",
    "            tag_set.add(token[\"upos\"])\n",
    "    tags=sorted(list(tag_set))\n",
    "    tag_dict={\"PAD\":0}\n",
    "    #tag_dict ={}\n",
    "    for i,tag in enumerate(tags):\n",
    "        tag_dict[tag]=i+1\n",
    "    return tag_dict\n",
    "    \n",
    "tags = create_tags(train_sentences) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'PAD': 0,\n",
       " 'ADJ': 1,\n",
       " 'ADP': 2,\n",
       " 'ADV': 3,\n",
       " 'AUX': 4,\n",
       " 'CCONJ': 5,\n",
       " 'DET': 6,\n",
       " 'INTJ': 7,\n",
       " 'NOUN': 8,\n",
       " 'NUM': 9,\n",
       " 'PART': 10,\n",
       " 'PRON': 11,\n",
       " 'PROPN': 12,\n",
       " 'VERB': 13}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "IGNORE_TAG_INDEX = 0\n",
    "def create_data(sentences,vocab,tags,max_seq_len=50):\n",
    "    sents_idx=[]\n",
    "    sent_tags=[]\n",
    "    #present=0\n",
    "    #not_present=0\n",
    "    for sent in sentences:\n",
    "        sent_idx=[]\n",
    "        sent_tag=[]\n",
    "        for token in sent:\n",
    "            if (token[\"form\"].lower() in vocab):\n",
    "                sent_idx.append(vocab[token[\"form\"].lower()])\n",
    "            else:\n",
    "                sent_idx.append(vocab[\"<UNK>\"])\n",
    "            sent_tag.append(tags[token[\"upos\"]])\n",
    "        sents_idx.append(sent_idx)\n",
    "        sent_tags.append(sent_tag)\n",
    "    for i in range(len(sents_idx)):\n",
    "        if len(sents_idx[i]) < max_seq_len:\n",
    "            sents_idx[i]=sents_idx[i]+[vocab[\"<PAD>\"] for _ in range(max_seq_len - len(sents_idx[i]))]\n",
    "            sent_tags[i]=sent_tags[i]+[tags[\"PAD\"] for _ in range(max_seq_len - len(sent_tags[i]))]\n",
    "#     print(present)\n",
    "#     print(not_present)\n",
    "    return sents_idx,sent_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent_to_vector(sentence,max_seq_len=50):\n",
    "    tokens = sentence.split(\" \")\n",
    "    sent_idx=[]\n",
    "    for token in tokens:\n",
    "        if (token.lower() in vocab):\n",
    "            sent_idx.append(vocab[token.lower()])\n",
    "        else:\n",
    "            sent_idx.append(vocab[\"<UNK>\"])\n",
    "    for i in range(len(sent_idx)):\n",
    "        if len(sent_idx) < max_seq_len:\n",
    "            sent_idx=sent_idx+[vocab[\"<PAD>\"] for _ in range(max_seq_len - len(sent_idx))]\n",
    "    return sent_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[11085,\n",
       " 199,\n",
       " 34,\n",
       " 83,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_to_vector(\"Hi how are you\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train :4274 Dev : 586 Test : 586\n"
     ]
    }
   ],
   "source": [
    "x_train,y_train=create_data(train_sentences,vocab,tags,50)\n",
    "x_dev,y_dev=create_data(dev_sentences,vocab,tags,50)\n",
    "x_test,y_test=create_data(test_sentences,vocab,tags,50)\n",
    "print(\"Train :\"+str(len(x_train)) + \" Dev : \"+str(len(x_dev))+ \" Test : \"+str(len(x_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class POSDataSet(Dataset):\n",
    "    def __init__(self, x, y):\n",
    "        self.sent = torch.LongTensor(x)\n",
    "        self.sent_tags = torch.LongTensor(y)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.sent[idx], self.sent_tags[idx]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = POSDataSet(x_train,y_train)\n",
    "dev_dataset = POSDataSet(x_dev,y_dev)\n",
    "test_dataset = POSDataSet(x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE =32\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "dev_dataloader = DataLoader(dev_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[104,  34,   2,  ...,   0,   0,   0],\n",
       "        [ 43, 305,   9,  ...,   0,   0,   0],\n",
       "        [ 43, 410,   9,  ...,   0,   0,   0],\n",
       "        ...,\n",
       "        [ 43, 410,   9,  ...,   0,   0,   0],\n",
       "        [ 43, 305,   6,  ...,   0,   0,   0],\n",
       "        [ 54, 457, 287,  ...,   0,   0,   0]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset.sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The accuracy function has been implemented for you\n",
    "def accuracy(true, pred):\n",
    "  \"\"\"\n",
    "  Arguments:\n",
    "  - true:       a list of true label values (integers)\n",
    "  - pred:       a list of predicted label values (integers)\n",
    "\n",
    "  Output:\n",
    "  - accuracy:   the prediction accuracy\n",
    "  \"\"\"\n",
    "  true = np.array(true)\n",
    "  pred = np.array(pred)\n",
    "\n",
    "  num_correct = sum(true == pred)\n",
    "  num_total = len(true)\n",
    "  return num_correct / num_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def confusion_matrix(true, pred, num_tags):\n",
    "  \"\"\"\n",
    "  Arguments:\n",
    "  - true:       a list of true label values (integers)\n",
    "  - pred:       a list of predicted label values (integers)\n",
    "  - num_tags:   the number of possible tags\n",
    "                true and pred will both contain integers between\n",
    "                0 and num_tags - 1 (inclusive)\n",
    "\n",
    "  Output: \n",
    "  - confusion_matrix:   a (num_tags x num_tags) matrix of integers\n",
    "\n",
    "  confusion_matrix[i][j] = # predictions where true label\n",
    "  was i and predicted label was j\n",
    "\n",
    "  \"\"\"\n",
    "\n",
    "  confusion_matrix = np.zeros((num_tags, num_tags))\n",
    "\n",
    "  #############################\n",
    "  \"\"\"\n",
    "  for tag in np.arange(len(num_tags)):\n",
    "    tp.append(sum(true == pred == tag))\n",
    "    fp.append(sum(pred == tag and true != tag))\n",
    "    fn.append(sum(pred != tag and true == tag))\n",
    "  \n",
    "  for i in np.arange(num_tags):\n",
    "    for j in np.arange(num_tags):\n",
    "      if i == j:\n",
    "        confusion_matrix[i][j] = sum(np.logical_and(true == i, pred == i))\n",
    "      else:\n",
    "        confusion_matrix[i][j] = sum(np.logical_and(pred == j, true == i))\n",
    "  \"\"\"\n",
    "  for t in np.arange(len(true)):\n",
    "    i = true[t]\n",
    "    j = pred[t]\n",
    "    confusion_matrix[i][j] += 1  \n",
    "  #############################\n",
    "  return confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision(true, pred, num_tags):\n",
    "  \"\"\"\n",
    "  Arguments:\n",
    "  - true:       a list of true label values (integers)\n",
    "  - pred:       a list of predicted label values (integers)\n",
    "  - num_tags:   the number of possible tags\n",
    "                true and pred will both contain integers between\n",
    "                0 and num_tags - 1 (inclusive)\n",
    "\n",
    "  Output: \n",
    "  - precision:  an array of length num_tags, where precision[i]\n",
    "                gives the precision of class i\n",
    "\n",
    "  Hints:  the confusion matrix may be useful\n",
    "          be careful about zero division\n",
    "  \"\"\"\n",
    "\n",
    "  precision = np.zeros(num_tags)\n",
    "\n",
    "  #############################\n",
    "  cm = confusion_matrix(true, pred, num_tags)\n",
    "  for i in np.arange(num_tags):\n",
    "    if (sum([cm[x][i] for x in np.arange(num_tags)])==0):\n",
    "      precision[i] = 0\n",
    "    else:\n",
    "      precision[i] = cm[i][i]/sum([cm[x][i] for x in np.arange(num_tags)])\n",
    "  #############################\n",
    "  return precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recall(true, pred, num_tags):\n",
    "  \"\"\"\n",
    "  Arguments:\n",
    "  - true:       a list of true label values (integers)\n",
    "  - pred:       a list of predicted label values (integers)\n",
    "  - num_tags:   the number of possible tags\n",
    "                true and pred will both contain integers between\n",
    "                0 and num_tags - 1 (inclusive)\n",
    "\n",
    "  Output: \n",
    "  - recall:     an array of length num_tags, where recall[i]\n",
    "                gives the recall of class i\n",
    "\n",
    "  Hints:  the confusion matrix may be useful\n",
    "          be careful about zero division\n",
    "  \"\"\"\n",
    "\n",
    "  \"\"\"\n",
    "  YOUR CODE HERE\n",
    "  \"\"\"\n",
    "  recall = np.zeros(num_tags)\n",
    "\n",
    "  #############################\n",
    "  cm = confusion_matrix(true, pred, num_tags)\n",
    "  for i in np.arange(num_tags):\n",
    "    if (sum([cm[i][x] for x in np.arange(num_tags)])==0):\n",
    "      recall[i] = 0\n",
    "    else:\n",
    "      recall[i] = cm[i][i]/sum([cm[i][x] for x in np.arange(num_tags)])\n",
    "  #############################\n",
    "  return recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1_score(true, pred, num_tags):\n",
    "  \"\"\"\n",
    "  Arguments:\n",
    "  - true:       a list of true label values (integers)\n",
    "  - pred:       a list of predicted label values (integers)\n",
    "  - num_tags:   the number of possible tags\n",
    "                true and pred will both contain integers between\n",
    "                0 and num_tags - 1 (inclusive)\n",
    "\n",
    "  Output: \n",
    "  - f1:         an array of length num_tags, where f1[i]\n",
    "                gives the recall of class i\n",
    "  \"\"\"\n",
    "  f1 = np.zeros(num_tags)\n",
    "\n",
    "  #############################\n",
    "  p = precision(true, pred, num_tags)\n",
    "  r = recall(true, pred, num_tags)\n",
    "\n",
    "  for i in np.arange(num_tags):\n",
    "    if p[i]+r[i] == 0:\n",
    "      f1[i] = 0\n",
    "    else:\n",
    "      f1[i]= (2*(p[i]*r[i])/(p[i]+r[i]))\n",
    "  #############################\n",
    "  return f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "  \"\"\"\n",
    "  Sets random seeds and sets model in deterministic\n",
    "  training mode. Ensures reproducible results\n",
    "  \"\"\"\n",
    "  torch.manual_seed(seed)\n",
    "  torch.backends.cudnn.deterministic = True\n",
    "  torch.backends.cudnn.benchmark = False\n",
    "  np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class POSTagger(nn.Module):\n",
    "    def __init__(self,max_seq_len,embeddings,hidden_dim,n_layers,tagset_size):\n",
    "        super().__init__()\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.num_labels = tagset_size\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.embeddings = nn.Embedding.from_pretrained(embeddings,padding_idx=0)\n",
    "        self.lstm = nn.LSTM(input_size=embeddings.size()[1], hidden_size= self.hidden_dim , num_layers=n_layers)\n",
    "        self.hidden2tag = nn.Linear(self.hidden_dim,self.num_labels)\n",
    "        \n",
    "    def forward(self,input_seq):\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        input_seq = input_seq.to(device)\n",
    "        embed_out =self.embeddings(input_seq)\n",
    "        #packed_embeddings = pack_padded_sequence(embed_out, self.max_seq_len, batch_first=True, enforce_sorted=False)\n",
    "        lstm_out,_ = self.lstm(embed_out)\n",
    "        #padded_output, _ = pad_packed_sequence(lstm_out, batch_first=True)\n",
    "        logits = self.hidden2tag(lstm_out)\n",
    "        return logits\n",
    "    \n",
    "    def evaluate(self,loader):\n",
    "        self.eval()\n",
    "        true_labels = []\n",
    "        pred_labels = []\n",
    "        for i, data in enumerate(loader):\n",
    "            x,y = data\n",
    "            logits = self.forward(x)\n",
    "            pred_label=torch.argmax(logits, dim=-1).cpu().numpy()\n",
    "            batch_size, _ = x.shape\n",
    "            for j in range(batch_size):\n",
    "                tags = y[j]\n",
    "                pred = pred_label[j]\n",
    "                for k in range(len(tags)):\n",
    "                    if tags[k] != 0:\n",
    "                        true_labels.append(tags[k])\n",
    "                        pred_labels.append(pred[k])\n",
    "        acc = accuracy(true_labels, pred_labels)  \n",
    "        return acc ,true_labels ,pred_labels          \n",
    "\n",
    "    def run_training(self,train_loader,dev_loader,epochs=100,learning_rate=5e-4,eval_every=5):\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        if str(device) == 'cpu':\n",
    "            print(\"Training only supported in GPU environment\")\n",
    "            return\n",
    "        torch.cuda.empty_cache()\n",
    "        self.to(device)\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=learning_rate)\n",
    "        loss_function = nn.CrossEntropyLoss(ignore_index=0)\n",
    "        for epoch in range(epochs):\n",
    "            self.train()\n",
    "            total_loss = 0\n",
    "            for i, data in enumerate(train_loader):\n",
    "                x,y=data\n",
    "                self.zero_grad()\n",
    "                logits = self.forward(x)\n",
    "                labels = torch.LongTensor(y).to(device)\n",
    "                loss = loss_function(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "                total_loss += loss\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            print(\"Epoch {} | Loss: {}\".format(epoch, total_loss))\n",
    "            if epoch % eval_every == 0:\n",
    "                acc,_,_ = self.evaluate(dev_loader)\n",
    "                print(\"Epoch {} | Accuracy: {}\".format(epoch, acc))         \n",
    "    \n",
    "    def predict(self,data):\n",
    "        x = torch.LongTensor(data)\n",
    "        self.eval()\n",
    "        predictions = []\n",
    "        logits = self.forward(x)\n",
    "        pred_label=torch.argmax(logits, dim=-1).cpu().numpy()\n",
    "        batch_size, _ = x.shape\n",
    "        for j in range(batch_size):\n",
    "            labels=[]\n",
    "            for k in range(len(x[j])):\n",
    "                if x[j][k] != 0:\n",
    "                    labels.append(pred_label[j][k])\n",
    "            predictions.append(labels)\n",
    "        return predictions\n",
    "\n",
    "    \n",
    "    def save(self,path):\n",
    "        torch.save(self.state_dict(), path)\n",
    "\n",
    "    def load(self,path):\n",
    "        self.load_state_dict(torch.load(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 | Loss: 238.99752807617188\n",
      "Epoch 0 | Accuracy: 0.6560790273556231\n",
      "Epoch 1 | Loss: 115.08439636230469\n",
      "Epoch 2 | Loss: 71.01158142089844\n",
      "Epoch 3 | Loss: 51.02167510986328\n",
      "Epoch 4 | Loss: 40.90391159057617\n",
      "Epoch 5 | Loss: 35.04903793334961\n",
      "Epoch 5 | Accuracy: 0.9293313069908815\n",
      "Epoch 6 | Loss: 31.07115936279297\n",
      "Epoch 7 | Loss: 28.149417877197266\n",
      "Epoch 8 | Loss: 25.518945693969727\n",
      "Epoch 9 | Loss: 23.328821182250977\n",
      "Epoch 10 | Loss: 21.715444564819336\n",
      "Epoch 10 | Accuracy: 0.9519756838905775\n",
      "Epoch 11 | Loss: 20.4924373626709\n",
      "Epoch 12 | Loss: 19.319236755371094\n",
      "Epoch 13 | Loss: 18.61332893371582\n",
      "Epoch 14 | Loss: 17.782766342163086\n",
      "Epoch 15 | Loss: 17.13642692565918\n",
      "Epoch 15 | Accuracy: 0.95790273556231\n",
      "Epoch 16 | Loss: 16.565166473388672\n",
      "Epoch 17 | Loss: 16.338045120239258\n",
      "Epoch 18 | Loss: 15.776994705200195\n",
      "Epoch 19 | Loss: 15.49870777130127\n",
      "Epoch 20 | Loss: 15.151619911193848\n",
      "Epoch 20 | Accuracy: 0.9604863221884499\n",
      "Epoch 21 | Loss: 14.860315322875977\n",
      "Epoch 22 | Loss: 14.746910095214844\n",
      "Epoch 23 | Loss: 14.679615020751953\n",
      "Epoch 24 | Loss: 14.237421035766602\n",
      "Epoch 25 | Loss: 14.14285659790039\n",
      "Epoch 25 | Accuracy: 0.9609422492401216\n",
      "Epoch 26 | Loss: 13.829431533813477\n",
      "Epoch 27 | Loss: 13.648028373718262\n",
      "Epoch 28 | Loss: 13.641332626342773\n",
      "Epoch 29 | Loss: 13.281021118164062\n",
      "Epoch 30 | Loss: 13.197212219238281\n",
      "Epoch 30 | Accuracy: 0.9626139817629179\n",
      "Epoch 31 | Loss: 13.090434074401855\n",
      "Epoch 32 | Loss: 13.006053924560547\n",
      "Epoch 33 | Loss: 12.883552551269531\n",
      "Epoch 34 | Loss: 12.653836250305176\n",
      "Epoch 35 | Loss: 12.59343433380127\n",
      "Epoch 35 | Accuracy: 0.9639817629179331\n",
      "Epoch 36 | Loss: 12.474390983581543\n",
      "Epoch 37 | Loss: 12.442249298095703\n",
      "Epoch 38 | Loss: 12.33851146697998\n",
      "Epoch 39 | Loss: 12.369068145751953\n",
      "Epoch 40 | Loss: 12.293401718139648\n",
      "Epoch 40 | Accuracy: 0.9639817629179331\n",
      "Epoch 41 | Loss: 11.962032318115234\n",
      "Epoch 42 | Loss: 12.094131469726562\n",
      "Epoch 43 | Loss: 11.938112258911133\n",
      "Epoch 44 | Loss: 11.916192054748535\n",
      "Epoch 45 | Loss: 11.926074028015137\n",
      "Epoch 45 | Accuracy: 0.9632218844984802\n",
      "Epoch 46 | Loss: 11.919183731079102\n",
      "Epoch 47 | Loss: 11.594476699829102\n",
      "Epoch 48 | Loss: 11.540884971618652\n",
      "Epoch 49 | Loss: 11.569000244140625\n"
     ]
    }
   ],
   "source": [
    "set_seed(159)\n",
    "tagger = POSTagger(50,embeddings,128,2,len(tags))\n",
    "tagger.run_training(train_dataloader,dev_dataloader,50,0.0005,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagger.save(\"pos_tagger.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9650455927051672\n"
     ]
    }
   ],
   "source": [
    "acc,_,_=tagger.evaluate(test_dataloader)\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictor(model,sentence,tags):\n",
    "    tokens = sentence.split(\" \")\n",
    "    inv_map = {v: k for k, v in tags.items()}\n",
    "    vec = [sent_to_vector(sentence)]\n",
    "    predictions = model.predict(vec)\n",
    "    for i in range(len(tokens)):\n",
    "        print (tokens[i]+\"\\t\"+inv_map[predictions[0][i]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i\tPRON\n",
      "want\tVERB\n",
      "a\tDET\n",
      "flight\tNOUN\n",
      "from\tADP\n",
      "nashville\tPROPN\n",
      "to\tADP\n",
      "seattle\tPROPN\n",
      "that\tADP\n",
      "arrives\tVERB\n",
      "no\tDET\n",
      "later\tADV\n",
      "than\tADP\n",
      "3\tNUM\n",
      "pm\tNOUN\n"
     ]
    }
   ],
   "source": [
    "predictor(tagger,\"i want a flight from nashville to seattle that arrives no later than 3 pm\",tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'PAD': 0,\n",
       " 'ADJ': 1,\n",
       " 'ADP': 2,\n",
       " 'ADV': 3,\n",
       " 'AUX': 4,\n",
       " 'CCONJ': 5,\n",
       " 'DET': 6,\n",
       " 'INTJ': 7,\n",
       " 'NOUN': 8,\n",
       " 'NUM': 9,\n",
       " 'PART': 10,\n",
       " 'PRON': 11,\n",
       " 'PROPN': 12,\n",
       " 'VERB': 13}"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tags"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
