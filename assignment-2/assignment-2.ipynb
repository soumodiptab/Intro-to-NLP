{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: conllu in /home/soumodiptab/.local/lib/python3.8/site-packages (4.5.2)\n"
     ]
    }
   ],
   "source": [
    "# Install dependencies\n",
    "!pip install conllu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import headers\n",
    "from conllu import parse,parse_incr\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence, pad_packed_sequence, pack_padded_sequence\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TokenList<what, is, the, cost, of, a, round, trip, flight, from, pittsburgh, to, atlanta, beginning, on, april, twenty, fifth, and, returning, on, may, sixth, metadata={sent_id: \"0001.train\", text: \"what is the cost of a round trip flight from pittsburgh to atlanta beginning on april twenty fifth and returning on may sixth\"}>\n"
     ]
    }
   ],
   "source": [
    "with open (\"data/UD_English-Atis/en_atis-ud-train.conllu\", \"r\", encoding=\"utf-8\") as f:\n",
    "    data = f.read()\n",
    "train_sentences = parse(data)\n",
    "with open (\"data/UD_English-Atis/en_atis-ud-test.conllu\", \"r\", encoding=\"utf-8\") as f:\n",
    "    data = f.read()\n",
    "dev_sentences = parse(data)\n",
    "with open (\"data/UD_English-Atis/en_atis-ud-test.conllu\", \"r\", encoding=\"utf-8\") as f:\n",
    "    data = f.read()\n",
    "test_sentences = parse(data)\n",
    "print(train_sentences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 2,\n",
       " 'form': 'is',\n",
       " 'lemma': 'be',\n",
       " 'upos': 'AUX',\n",
       " 'xpos': None,\n",
       " 'feats': {'Mood': 'Ind',\n",
       "  'Number': 'Sing',\n",
       "  'Person': '3',\n",
       "  'Tense': 'Pres',\n",
       "  'VerbForm': 'Fin'},\n",
       " 'head': 1,\n",
       " 'deprel': 'cop',\n",
       " 'deps': None,\n",
       " 'misc': None}"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = train_sentences[0]\n",
    "sentence[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_embeddings(filename, vocab_size=10000):\n",
    "    with open(filename, encoding=\"utf-8\") as file:\n",
    "        word_embedding_dim = len(file.readline().split(\" \")) - 1\n",
    "    vocab = {}\n",
    "    embeddings = np.zeros((vocab_size, word_embedding_dim))\n",
    "    with open(filename, encoding=\"utf-8\") as file:\n",
    "        for idx, line in enumerate(file):\n",
    "            if idx + 2 >= vocab_size:\n",
    "                break\n",
    "            cols = line.rstrip().split(\" \")\n",
    "            val = np.array(cols[1:])\n",
    "            word = cols[0]\n",
    "            embeddings[idx + 2] = val\n",
    "            vocab[word] = idx + 2\n",
    "    vocab[\"<UNK>\"]=1\n",
    "    vocab[\"<PAD>\"]=0\n",
    "  # a FloatTensor is a multidimensional matrix\n",
    "  # that contains 32-bit floats in every entry\n",
    "  # https://pytorch.org/docs/stable/tensors.html\n",
    "    return torch.FloatTensor(embeddings), vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 50000\n",
    "embeddings, vocab = read_embeddings('./data/glove.6B.100d.txt', vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence[0][\"form\"].lower() in vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocab(sentences):\n",
    "    data =[]\n",
    "    word_set = set()\n",
    "    vocab_dict={\"<PAD>\":0,\"<UNK>\":1}\n",
    "    for sent in sentences:\n",
    "        for token in sent:\n",
    "            word_set.add(token[\"form\"])\n",
    "    word_list = sorted(list(word_set))\n",
    "    for i,word in enumerate(word_list):\n",
    "        vocab_dict[word]=i+2\n",
    "    return vocab_dict\n",
    "#vocab_dict=build_vocab(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tags(sentences):\n",
    "    tag_set=set()\n",
    "    for sent in sentences:\n",
    "        for token in sent:\n",
    "            tag_set.add(token[\"upos\"])\n",
    "    tags=sorted(list(tag_set))\n",
    "    tag_dict={\"PAD\":0}\n",
    "    #tag_dict ={}\n",
    "    for i,tag in enumerate(tags):\n",
    "        tag_dict[tag]=i+1\n",
    "    return tag_dict\n",
    "    \n",
    "tags = create_tags(train_sentences) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'PAD': 0,\n",
       " 'ADJ': 1,\n",
       " 'ADP': 2,\n",
       " 'ADV': 3,\n",
       " 'AUX': 4,\n",
       " 'CCONJ': 5,\n",
       " 'DET': 6,\n",
       " 'INTJ': 7,\n",
       " 'NOUN': 8,\n",
       " 'NUM': 9,\n",
       " 'PART': 10,\n",
       " 'PRON': 11,\n",
       " 'PROPN': 12,\n",
       " 'VERB': 13}"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "IGNORE_TAG_INDEX = 0\n",
    "def create_data(sentences,vocab,tags,max_seq_len=50):\n",
    "    sents_idx=[]\n",
    "    sent_tags=[]\n",
    "    #present=0\n",
    "    #not_present=0\n",
    "    for sent in sentences:\n",
    "        sent_idx=[]\n",
    "        sent_tag=[]\n",
    "        for token in sent:\n",
    "            if (token[\"form\"].lower() in vocab):\n",
    "                sent_idx.append(vocab[token[\"form\"].lower()])\n",
    "            else:\n",
    "                sent_idx.append(vocab[\"<UNK>\"])\n",
    "            sent_tag.append(tags[token[\"upos\"]])\n",
    "        sents_idx.append(sent_idx)\n",
    "        sent_tags.append(sent_tag)\n",
    "    for i in range(len(sents_idx)):\n",
    "        if len(sents_idx[i]) < max_seq_len:\n",
    "            sents_idx[i]=sents_idx[i]+[vocab[\"<PAD>\"] for _ in range(max_seq_len - len(sents_idx[i]))]\n",
    "            sent_tags[i]=sent_tags[i]+[tags[\"PAD\"] for _ in range(max_seq_len - len(sent_tags[i]))]\n",
    "#     print(present)\n",
    "#     print(not_present)\n",
    "    return sents_idx,sent_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent_to_vector(sentence,max_seq_len=50):\n",
    "    tokens = sentence.split(\" \")\n",
    "    sent_idx=[]\n",
    "    for token in tokens:\n",
    "        if (token.lower() in vocab):\n",
    "            sent_idx.append(vocab[token.lower()])\n",
    "        else:\n",
    "            sent_idx.append(vocab[\"<UNK>\"])\n",
    "    for i in range(len(sent_idx)):\n",
    "        if len(sent_idx) < max_seq_len:\n",
    "            sent_idx=sent_idx+[vocab[\"<PAD>\"] for _ in range(max_seq_len - len(sent_idx))]\n",
    "    return sent_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[11085,\n",
       " 199,\n",
       " 34,\n",
       " 83,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_to_vector(\"Hi how are you\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train :4274 Dev : 586 Test : 586\n"
     ]
    }
   ],
   "source": [
    "x_train,y_train=create_data(train_sentences,vocab,tags,50)\n",
    "x_dev,y_dev=create_data(dev_sentences,vocab,tags,50)\n",
    "x_test,y_test=create_data(test_sentences,vocab,tags,50)\n",
    "print(\"Train :\"+str(len(x_train)) + \" Dev : \"+str(len(x_dev))+ \" Test : \"+str(len(x_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "class POSDataSet(Dataset):\n",
    "    def __init__(self, x, y):\n",
    "        self.sent = torch.LongTensor(x)\n",
    "        self.sent_tags = torch.LongTensor(y)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.sent[idx], self.sent_tags[idx]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = POSDataSet(x_train,y_train)\n",
    "dev_dataset = POSDataSet(x_dev,y_dev)\n",
    "test_dataset = POSDataSet(x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE =32\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "dev_dataloader = DataLoader(dev_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[104,  34,   2,  ...,   0,   0,   0],\n",
       "        [ 43, 305,   9,  ...,   0,   0,   0],\n",
       "        [ 43, 410,   9,  ...,   0,   0,   0],\n",
       "        ...,\n",
       "        [ 43, 410,   9,  ...,   0,   0,   0],\n",
       "        [ 43, 305,   6,  ...,   0,   0,   0],\n",
       "        [ 54, 457, 287,  ...,   0,   0,   0]])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset.sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The accuracy function has been implemented for you\n",
    "def accuracy(true, pred):\n",
    "  \"\"\"\n",
    "  Arguments:\n",
    "  - true:       a list of true label values (integers)\n",
    "  - pred:       a list of predicted label values (integers)\n",
    "\n",
    "  Output:\n",
    "  - accuracy:   the prediction accuracy\n",
    "  \"\"\"\n",
    "  true = np.array(true)\n",
    "  pred = np.array(pred)\n",
    "\n",
    "  num_correct = sum(true == pred)\n",
    "  num_total = len(true)\n",
    "  return num_correct / num_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "  \"\"\"\n",
    "  Sets random seeds and sets model in deterministic\n",
    "  training mode. Ensures reproducible results\n",
    "  \"\"\"\n",
    "  torch.manual_seed(seed)\n",
    "  torch.backends.cudnn.deterministic = True\n",
    "  torch.backends.cudnn.benchmark = False\n",
    "  np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "class POSTagger(nn.Module):\n",
    "    def __init__(self,max_seq_len,embeddings,hidden_dim,n_layers,tagset_size):\n",
    "        super().__init__()\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.num_labels = tagset_size\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.embeddings = nn.Embedding.from_pretrained(embeddings,padding_idx=0)\n",
    "        self.lstm = nn.LSTM(input_size=embeddings.size()[1], hidden_size= self.hidden_dim , num_layers=n_layers)\n",
    "        self.hidden2tag = nn.Linear(self.hidden_dim,self.num_labels)\n",
    "        \n",
    "    def forward(self,input_seq):\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        input_seq = input_seq.to(device)\n",
    "        embed_out =self.embeddings(input_seq)\n",
    "        lstm_out,_ = self.lstm(embed_out)\n",
    "        logits = self.hidden2tag(lstm_out)\n",
    "        return logits\n",
    "    \n",
    "    def evaluate(self,loader):\n",
    "        self.eval()\n",
    "        true_labels = []\n",
    "        pred_labels = []\n",
    "        for i, data in enumerate(loader):\n",
    "            x,y = data\n",
    "            logits = self.forward(x)\n",
    "            pred_label=torch.argmax(logits, dim=-1).cpu().numpy()\n",
    "            batch_size, _ = x.shape\n",
    "            for j in range(batch_size):\n",
    "                tags = y[j]\n",
    "                pred = pred_label[j]\n",
    "                for k in range(len(tags)):\n",
    "                    if tags[k] != 0:\n",
    "                        true_labels.append(tags[k])\n",
    "                        pred_labels.append(pred[k])\n",
    "        acc = accuracy(true_labels, pred_labels)  \n",
    "        return acc ,true_labels ,pred_labels          \n",
    "\n",
    "    def run_training(self,train_loader,dev_loader,epochs=100,learning_rate=5e-4,eval_every=5):\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        if str(device) == 'cpu':\n",
    "            print(\"Training only supported in GPU environment\")\n",
    "            return\n",
    "        torch.cuda.empty_cache()\n",
    "        self.to(device)\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=learning_rate)\n",
    "        loss_function = nn.CrossEntropyLoss(ignore_index=0)\n",
    "        for epoch in tqdm(range(epochs)):\n",
    "            self.train()\n",
    "            total_loss = 0\n",
    "            for i, data in enumerate(train_loader):\n",
    "                x,y=data\n",
    "                self.zero_grad()\n",
    "                logits = self.forward(x)\n",
    "                labels = torch.LongTensor(y).to(device)\n",
    "                loss = loss_function(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "                total_loss += loss\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            # print(\"Epoch {} | Loss: {}\".format(epoch, total_loss))\n",
    "            # if epoch % eval_every == eval_every-1:\n",
    "            #     acc,_,_ = self.evaluate(dev_loader)\n",
    "            #     print(\"Epoch {} | Accuracy: {}\".format(epoch, acc))\n",
    "        acc_train,_,_ = self.evaluate(train_loader)\n",
    "        acc_val,true_labels,pred_labels = self.evaluate(dev_loader)\n",
    "        print(\"# Model : Training Accuracy : {} Validation Accuracy: {} #\".format(acc_train,acc_val))  \n",
    "    \n",
    "    def predict(self,data):\n",
    "        x = torch.LongTensor(data)\n",
    "        self.eval()\n",
    "        predictions = []\n",
    "        logits = self.forward(x)\n",
    "        pred_label=torch.argmax(logits, dim=-1).cpu().numpy()\n",
    "        batch_size, _ = x.shape\n",
    "        for j in range(batch_size):\n",
    "            labels=[]\n",
    "            for k in range(len(x[j])):\n",
    "                if x[j][k] != 0:\n",
    "                    labels.append(pred_label[j][k])\n",
    "            predictions.append(labels)\n",
    "        return predictions\n",
    "\n",
    "    \n",
    "    def save(self,path):\n",
    "        torch.save(self.state_dict(), path)\n",
    "\n",
    "    def load(self,path):\n",
    "        self.load_state_dict(torch.load(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:32<00:00,  1.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Model : Training Accuracy : 0.9679580721405816 Validation Accuracy: 0.9673252279635258 #\n"
     ]
    }
   ],
   "source": [
    "set_seed(159)\n",
    "tagger = POSTagger(50,embeddings,128,2,len(tags))\n",
    "tagger.run_training(train_dataloader,dev_dataloader,50,0.0005,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagger.save(\"pos_tagger.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9673252279635258\n"
     ]
    }
   ],
   "source": [
    "acc,_,_=tagger.evaluate(test_dataloader)\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictor(model : POSTagger,sentence,tags):\n",
    "    tokens = sentence.split(\" \")\n",
    "    inv_map = {v: k for k, v in tags.items()}\n",
    "    vec = [sent_to_vector(sentence)]\n",
    "    predictions = model.predict(vec)\n",
    "    for i in range(len(tokens)):\n",
    "        print (tokens[i]+\"\\t\"+inv_map[predictions[0][i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mary\tPROPN\n",
      "likes\tVERB\n",
      "icecream\tPROPN\n"
     ]
    }
   ],
   "source": [
    "predictor(tagger,\"Mary likes icecream\",tags)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyper Parameter Tuning and model exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyzer(train_dataloader,dev_dataloader,test_dataloader,tags):\n",
    "    lrs = [0.0001,0.0005]\n",
    "    layers = [1,2]\n",
    "    hidden_dims = [32,64,128,256,512]\n",
    "    best_accuracy = 0\n",
    "    best_config = {\"lr\":0,\"layers\":0,\"hidden_dim\":0}\n",
    "    for lr in lrs:\n",
    "        for layer in layers:\n",
    "            for hidden_dim in hidden_dims:\n",
    "                set_seed(159)\n",
    "                tagger = POSTagger(50,embeddings,hidden_dim,layer,len(tags))\n",
    "                tagger.run_training(train_dataloader,dev_dataloader,50,lr,5)\n",
    "                acc,true_labels,pred_labels=tagger.evaluate(test_dataloader)\n",
    "                #score = f1_score(true_labels,pred_labels,len(tags))\n",
    "                #cf = confusion_matrix(true_labels,pred_labels,len(tags))\n",
    "                print(\"------------------------------------------------------------------\")\n",
    "                print(\"# Model Parameters | Learning Rate : {} Layers : {} Hidden Dim : {} #\".format(lr,layer,hidden_dim))\n",
    "                print(\"# Analysis | : Accuracy : {} #\".format(acc))\n",
    "                # print(\"# Confusion Matrix : # : \")\n",
    "                # print(cf)\n",
    "                print(\"------------------------------------------------------------------\")\n",
    "                if acc > best_accuracy:\n",
    "                    tagger.save(\"pos_tagger.pt\")\n",
    "                    best_accuracy = acc\n",
    "                    best_config = {\"lr\":lr,\"layers\":layer,\"hidden_dim\":hidden_dim}\n",
    "                \n",
    "    print(\"Best Accuracy : {} Best Config : {}\".format(best_accuracy,best_config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:21<00:00,  2.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Model : Training Accuracy : 0.9444250333984174 Validation Accuracy: 0.9430091185410334 #\n",
      "------------------------------------------------------------------\n",
      "# Model Parameters | Learning Rate : 0.0001 Layers : 1 Hidden Dim : 32 #\n",
      "# Analysis | : Accuracy : 0.9430091185410334 #\n",
      "------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:21<00:00,  2.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Model : Training Accuracy : 0.9526872880485048 Validation Accuracy: 0.952887537993921 #\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------\n",
      "# Model Parameters | Learning Rate : 0.0001 Layers : 1 Hidden Dim : 64 #\n",
      "# Analysis | : Accuracy : 0.952887537993921 #\n",
      "------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:23<00:00,  2.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Model : Training Accuracy : 0.9576816360086322 Validation Accuracy: 0.9592705167173252 #\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------\n",
      "# Model Parameters | Learning Rate : 0.0001 Layers : 1 Hidden Dim : 128 #\n",
      "# Analysis | : Accuracy : 0.9592705167173252 #\n",
      "------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:34<00:00,  1.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Model : Training Accuracy : 0.9625732196074401 Validation Accuracy: 0.9632218844984802 #\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------\n",
      "# Model Parameters | Learning Rate : 0.0001 Layers : 1 Hidden Dim : 256 #\n",
      "# Analysis | : Accuracy : 0.9632218844984802 #\n",
      "------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:39<00:00,  1.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Model : Training Accuracy : 0.9629020655636625 Validation Accuracy: 0.963677811550152 #\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------\n",
      "# Model Parameters | Learning Rate : 0.0001 Layers : 1 Hidden Dim : 512 #\n",
      "# Analysis | : Accuracy : 0.963677811550152 #\n",
      "------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:31<00:00,  1.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Model : Training Accuracy : 0.9421025588325969 Validation Accuracy: 0.9407294832826748 #\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------\n",
      "# Model Parameters | Learning Rate : 0.0001 Layers : 2 Hidden Dim : 32 #\n",
      "# Analysis | : Accuracy : 0.9407294832826748 #\n",
      "------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:34<00:00,  1.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Model : Training Accuracy : 0.9567978625012845 Validation Accuracy: 0.9566869300911854 #\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------\n",
      "# Model Parameters | Learning Rate : 0.0001 Layers : 2 Hidden Dim : 64 #\n",
      "# Analysis | : Accuracy : 0.9566869300911854 #\n",
      "------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:38<00:00,  1.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Model : Training Accuracy : 0.960970095570856 Validation Accuracy: 0.9601823708206687 #\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------\n",
      "# Model Parameters | Learning Rate : 0.0001 Layers : 2 Hidden Dim : 128 #\n",
      "# Analysis | : Accuracy : 0.9601823708206687 #\n",
      "------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:51<00:00,  1.02s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Model : Training Accuracy : 0.9641968965162881 Validation Accuracy: 0.9630699088145896 #\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------\n",
      "# Model Parameters | Learning Rate : 0.0001 Layers : 2 Hidden Dim : 256 #\n",
      "# Analysis | : Accuracy : 0.9630699088145896 #\n",
      "------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [01:12<00:00,  1.45s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Model : Training Accuracy : 0.9637652861987462 Validation Accuracy: 0.9629179331306991 #\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------\n",
      "# Model Parameters | Learning Rate : 0.0001 Layers : 2 Hidden Dim : 512 #\n",
      "# Analysis | : Accuracy : 0.9629179331306991 #\n",
      "------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:25<00:00,  1.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Model : Training Accuracy : 0.9643818723666633 Validation Accuracy: 0.9606382978723405 #\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------\n",
      "# Model Parameters | Learning Rate : 0.0005 Layers : 1 Hidden Dim : 32 #\n",
      "# Analysis | : Accuracy : 0.9606382978723405 #\n",
      "------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:23<00:00,  2.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Model : Training Accuracy : 0.967074298633234 Validation Accuracy: 0.9661094224924012 #\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------\n",
      "# Model Parameters | Learning Rate : 0.0005 Layers : 1 Hidden Dim : 64 #\n",
      "# Analysis | : Accuracy : 0.9661094224924012 #\n",
      "------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:27<00:00,  1.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Model : Training Accuracy : 0.9672181687390813 Validation Accuracy: 0.9659574468085106 #\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------\n",
      "# Model Parameters | Learning Rate : 0.0005 Layers : 1 Hidden Dim : 128 #\n",
      "# Analysis | : Accuracy : 0.9659574468085106 #\n",
      "------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:35<00:00,  1.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Model : Training Accuracy : 0.9679991778851095 Validation Accuracy: 0.9651975683890578 #\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------\n",
      "# Model Parameters | Learning Rate : 0.0005 Layers : 1 Hidden Dim : 256 #\n",
      "# Analysis | : Accuracy : 0.9651975683890578 #\n",
      "------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:44<00:00,  1.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Model : Training Accuracy : 0.9673414859726647 Validation Accuracy: 0.9664133738601823 #\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------\n",
      "# Model Parameters | Learning Rate : 0.0005 Layers : 1 Hidden Dim : 512 #\n",
      "# Analysis | : Accuracy : 0.9664133738601823 #\n",
      "------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:35<00:00,  1.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Model : Training Accuracy : 0.9658411262974 Validation Accuracy: 0.9630699088145896 #\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------\n",
      "# Model Parameters | Learning Rate : 0.0005 Layers : 2 Hidden Dim : 32 #\n",
      "# Analysis | : Accuracy : 0.9630699088145896 #\n",
      "------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:31<00:00,  1.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Model : Training Accuracy : 0.9673620388449286 Validation Accuracy: 0.9671732522796352 #\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------\n",
      "# Model Parameters | Learning Rate : 0.0005 Layers : 2 Hidden Dim : 64 #\n",
      "# Analysis | : Accuracy : 0.9671732522796352 #\n",
      "------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:37<00:00,  1.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Model : Training Accuracy : 0.9679580721405816 Validation Accuracy: 0.9673252279635258 #\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------\n",
      "# Model Parameters | Learning Rate : 0.0005 Layers : 2 Hidden Dim : 128 #\n",
      "# Analysis | : Accuracy : 0.9673252279635258 #\n",
      "------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:55<00:00,  1.11s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Model : Training Accuracy : 0.9685746583084986 Validation Accuracy: 0.9670212765957447 #\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------\n",
      "# Model Parameters | Learning Rate : 0.0005 Layers : 2 Hidden Dim : 256 #\n",
      "# Analysis | : Accuracy : 0.9670212765957447 #\n",
      "------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [01:11<00:00,  1.43s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Model : Training Accuracy : 0.9681841537354845 Validation Accuracy: 0.9656534954407295 #\n",
      "------------------------------------------------------------------\n",
      "# Model Parameters | Learning Rate : 0.0005 Layers : 2 Hidden Dim : 512 #\n",
      "# Analysis | : Accuracy : 0.9656534954407295 #\n",
      "------------------------------------------------------------------\n",
      "Best Accuracy : 0.9673252279635258 Best Config : {'lr': 0.0005, 'layers': 2, 'hidden_dim': 128}\n"
     ]
    }
   ],
   "source": [
    "analyzer(train_dataloader,dev_dataloader,test_dataloader,tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tag_analysis(data_loader,model: POSTagger,tags):\n",
    "    inv_map = {v: k for k, v in tags.items()}\n",
    "    acc,true_labels,pred_labels=model.evaluate(data_loader)\n",
    "    print(classification_report(true_labels,pred_labels,labels=list(tags.values()),target_names=list(tags.keys())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         PAD       0.00      0.00      0.00         0\n",
      "         ADJ       0.95      0.96      0.95       220\n",
      "         ADP       0.97      1.00      0.98      1434\n",
      "         ADV       0.91      0.79      0.85        76\n",
      "         AUX       0.96      0.99      0.97       256\n",
      "       CCONJ       1.00      1.00      1.00       109\n",
      "         DET       0.99      0.86      0.92       512\n",
      "        INTJ       0.95      1.00      0.97        36\n",
      "        NOUN       0.99      0.98      0.98      1166\n",
      "         NUM       0.94      0.95      0.95       127\n",
      "        PART       0.84      0.29      0.43        56\n",
      "        PRON       0.84      0.98      0.91       392\n",
      "       PROPN       0.98      0.99      0.99      1567\n",
      "        VERB       0.98      0.97      0.97       629\n",
      "\n",
      "   micro avg       0.97      0.97      0.97      6580\n",
      "   macro avg       0.88      0.84      0.85      6580\n",
      "weighted avg       0.97      0.97      0.97      6580\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/soumodiptab/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/soumodiptab/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/soumodiptab/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/soumodiptab/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/soumodiptab/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/soumodiptab/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "tag_analysis(test_dataloader,tagger,tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImprovedPOSTagger(POSTagger):\n",
    "    def __init__(self,max_seq_len,embeddings,hidden_dim,n_layers,tagset_size,dropout=0.8,bidirectional=True):\n",
    "        super().__init__(max_seq_len,embeddings,hidden_dim,n_layers,tagset_size)\n",
    "        self.lstm = nn.LSTM(input_size=embeddings.size()[1], hidden_size=hidden_dim, dropout=dropout, num_layers=n_layers, bidirectional=bidirectional)\n",
    "        self.hidden2tag = nn.Linear(self.hidden_dim*2,self.num_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [01:37<00:00,  1.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Model : Training Accuracy : 0.9690884801150961 Validation Accuracy: 0.9679331306990882 #\n"
     ]
    }
   ],
   "source": [
    "tagger2 = ImprovedPOSTagger(50,embeddings,128,2,len(tags))\n",
    "tagger2.run_training(train_dataloader,dev_dataloader,100,0.0005,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagger2.save('pos_tagger2.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy for improved model: 0.9679331306990882\n"
     ]
    }
   ],
   "source": [
    "acc,_,_=tagger2.evaluate(test_dataloader)\n",
    "print(\"Test accuracy for improved model: \"+str(acc))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
