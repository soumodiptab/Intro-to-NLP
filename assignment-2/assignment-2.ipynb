{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: conllu in /home/soumodiptab/.local/lib/python3.8/site-packages (4.5.2)\n"
     ]
    }
   ],
   "source": [
    "# Install dependencies\n",
    "!pip install conllu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import headers\n",
    "from conllu import parse,parse_incr\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence, pad_packed_sequence, pack_padded_sequence\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TokenList<what, is, the, cost, of, a, round, trip, flight, from, pittsburgh, to, atlanta, beginning, on, april, twenty, fifth, and, returning, on, may, sixth, metadata={sent_id: \"0001.train\", text: \"what is the cost of a round trip flight from pittsburgh to atlanta beginning on april twenty fifth and returning on may sixth\"}>\n"
     ]
    }
   ],
   "source": [
    "with open (\"data/UD_English-Atis/en_atis-ud-train.conllu\", \"r\", encoding=\"utf-8\") as f:\n",
    "    data = f.read()\n",
    "train_sentences = parse(data)\n",
    "with open (\"data/UD_English-Atis/en_atis-ud-dev.conllu\", \"r\", encoding=\"utf-8\") as f:\n",
    "    data = f.read()\n",
    "dev_sentences = parse(data)\n",
    "with open (\"data/UD_English-Atis/en_atis-ud-test.conllu\", \"r\", encoding=\"utf-8\") as f:\n",
    "    data = f.read()\n",
    "test_sentences = parse(data)\n",
    "print(train_sentences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 2,\n",
       " 'form': 'is',\n",
       " 'lemma': 'be',\n",
       " 'upos': 'AUX',\n",
       " 'xpos': None,\n",
       " 'feats': {'Mood': 'Ind',\n",
       "  'Number': 'Sing',\n",
       "  'Person': '3',\n",
       "  'Tense': 'Pres',\n",
       "  'VerbForm': 'Fin'},\n",
       " 'head': 1,\n",
       " 'deprel': 'cop',\n",
       " 'deps': None,\n",
       " 'misc': None}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = train_sentences[0]\n",
    "sentence[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_uniq_words(sentences):\n",
    "    words = set()\n",
    "    for sentence in sentences:\n",
    "        for token in sentence:\n",
    "            words.add(token[\"form\"].lower())\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "uniq = get_uniq_words(train_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_embeddings(filename, vocab_size=10000,uniq_words=None):\n",
    "    with open(filename, encoding=\"utf-8\") as file:\n",
    "        word_embedding_dim = len(file.readline().split(\" \")) - 1\n",
    "    vocab = {}\n",
    "    embeddings = np.zeros((vocab_size, word_embedding_dim))\n",
    "    last_idx = 0\n",
    "    with open(filename, encoding=\"utf-8\") as file:\n",
    "        for idx, line in enumerate(file):\n",
    "            if idx + 2 >= vocab_size:\n",
    "                break\n",
    "            cols = line.rstrip().split(\" \")\n",
    "            val = np.array(cols[1:])\n",
    "            word = cols[0]\n",
    "            embeddings[idx + 2] = val\n",
    "            vocab[word] = idx + 2\n",
    "            last_idx = idx + 2\n",
    "        # global_vocab ={}\n",
    "        # for idx, line in enumerate(file):\n",
    "        #     cols = line.rstrip().split(\" \")\n",
    "        #     val = np.array(cols[1:])\n",
    "        #     word = cols[0]\n",
    "        #     global_vocab[word] = val\n",
    "        \n",
    "        # for word in uniq_words:\n",
    "        #     if word not in vocab:\n",
    "        #         if word in global_vocab:\n",
    "        #             last_idx += 1\n",
    "        #             vocab[word] = last_idx\n",
    "        #             embeddings=np.append(embeddings,global_vocab[word])\n",
    "    vocab[\"<UNK>\"]=1\n",
    "    vocab[\"<PAD>\"]=0\n",
    "    return torch.FloatTensor(embeddings), vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 50000\n",
    "embeddings, vocab = read_embeddings('./data/glove.6B.100d.txt', vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence[0][\"form\"].lower() in vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocab(sentences):\n",
    "    data =[]\n",
    "    word_set = set()\n",
    "    vocab_dict={\"<PAD>\":0,\"<UNK>\":1}\n",
    "    for sent in sentences:\n",
    "        for token in sent:\n",
    "            word_set.add(token[\"form\"])\n",
    "    word_list = sorted(list(word_set))\n",
    "    for i,word in enumerate(word_list):\n",
    "        vocab_dict[word]=i+2\n",
    "    return vocab_dict\n",
    "#vocab_dict=build_vocab(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tags(sentences):\n",
    "    tag_set=set()\n",
    "    for sent in sentences:\n",
    "        for token in sent:\n",
    "            tag_set.add(token[\"upos\"])\n",
    "    tags=sorted(list(tag_set))\n",
    "    tag_dict={\"PAD\":0}\n",
    "    #tag_dict ={}\n",
    "    for i,tag in enumerate(tags):\n",
    "        tag_dict[tag]=i+1\n",
    "    return tag_dict\n",
    "\n",
    "total_sentences = train_sentences + dev_sentences + test_sentences    \n",
    "tags = create_tags(total_sentences) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'PAD': 0,\n",
       " 'ADJ': 1,\n",
       " 'ADP': 2,\n",
       " 'ADV': 3,\n",
       " 'AUX': 4,\n",
       " 'CCONJ': 5,\n",
       " 'DET': 6,\n",
       " 'INTJ': 7,\n",
       " 'NOUN': 8,\n",
       " 'NUM': 9,\n",
       " 'PART': 10,\n",
       " 'PRON': 11,\n",
       " 'PROPN': 12,\n",
       " 'SYM': 13,\n",
       " 'VERB': 14}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('tags.json', 'w') as fp:\n",
    "    json.dump(tags, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "IGNORE_TAG_INDEX = 0\n",
    "def create_data(sentences,vocab,tags,max_seq_len=50):\n",
    "    sents_idx=[]\n",
    "    sent_tags=[]\n",
    "    for sent in sentences:\n",
    "        sent_idx=[]\n",
    "        sent_tag=[]\n",
    "        for token in sent:\n",
    "            if (token[\"form\"].lower() in vocab):\n",
    "                sent_idx.append(vocab[token[\"form\"].lower()])\n",
    "            else:\n",
    "                sent_idx.append(vocab[\"<UNK>\"])\n",
    "            sent_tag.append(tags[token[\"upos\"]])\n",
    "        sents_idx.append(sent_idx)\n",
    "        sent_tags.append(sent_tag)\n",
    "    for i in range(len(sents_idx)):\n",
    "        if len(sents_idx[i]) < max_seq_len:\n",
    "            sents_idx[i]=sents_idx[i]+[vocab[\"<PAD>\"] for _ in range(max_seq_len - len(sents_idx[i]))]\n",
    "            sent_tags[i]=sent_tags[i]+[tags[\"PAD\"] for _ in range(max_seq_len - len(sent_tags[i]))]\n",
    "    return sents_idx,sent_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent_to_vector(sentence,max_seq_len=50):\n",
    "    tokens = sentence.split(\" \")\n",
    "    sent_idx=[]\n",
    "    for token in tokens:\n",
    "        if (token.lower() in vocab):\n",
    "            sent_idx.append(vocab[token.lower()])\n",
    "        else:\n",
    "            sent_idx.append(vocab[\"<UNK>\"])\n",
    "    for i in range(len(sent_idx)):\n",
    "        if len(sent_idx) < max_seq_len:\n",
    "            sent_idx=sent_idx+[vocab[\"<PAD>\"] for _ in range(max_seq_len - len(sent_idx))]\n",
    "    return sent_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[11085,\n",
       " 199,\n",
       " 34,\n",
       " 83,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_to_vector(\"Hi how are you\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train :4274 Dev : 572 Test : 586\n"
     ]
    }
   ],
   "source": [
    "x_train,y_train=create_data(train_sentences,vocab,tags,50)\n",
    "x_dev,y_dev=create_data(dev_sentences,vocab,tags,50)\n",
    "x_test,y_test=create_data(test_sentences,vocab,tags,50)\n",
    "print(\"Train :\"+str(len(x_train)) + \" Dev : \"+str(len(x_dev))+ \" Test : \"+str(len(x_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class POSDataSet(Dataset):\n",
    "    def __init__(self, x, y):\n",
    "        self.sent = torch.LongTensor(x)\n",
    "        self.sent_tags = torch.LongTensor(y)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.sent[idx], self.sent_tags[idx]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = POSDataSet(x_train,y_train)\n",
    "dev_dataset = POSDataSet(x_dev,y_dev)\n",
    "test_dataset = POSDataSet(x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE =32\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "dev_dataloader = DataLoader(dev_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[104,  34,   2,  ...,   0,   0,   0],\n",
       "        [ 43, 305,   9,  ...,   0,   0,   0],\n",
       "        [ 43, 410,   9,  ...,   0,   0,   0],\n",
       "        ...,\n",
       "        [ 43, 410,   9,  ...,   0,   0,   0],\n",
       "        [ 43, 305,   6,  ...,   0,   0,   0],\n",
       "        [ 54, 457, 287,  ...,   0,   0,   0]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset.sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The accuracy function has been implemented for you\n",
    "def accuracy(true, pred):\n",
    "  \"\"\"\n",
    "  Arguments:\n",
    "  - true:       a list of true label values (integers)\n",
    "  - pred:       a list of predicted label values (integers)\n",
    "\n",
    "  Output:\n",
    "  - accuracy:   the prediction accuracy\n",
    "  \"\"\"\n",
    "  true = np.array(true)\n",
    "  pred = np.array(pred)\n",
    "\n",
    "  num_correct = sum(true == pred)\n",
    "  num_total = len(true)\n",
    "  return num_correct / num_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "  \"\"\"\n",
    "  Sets random seeds and sets model in deterministic\n",
    "  training mode. Ensures reproducible results\n",
    "  \"\"\"\n",
    "  torch.manual_seed(seed)\n",
    "  torch.backends.cudnn.deterministic = True\n",
    "  torch.backends.cudnn.benchmark = False\n",
    "  np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class POSTagger(nn.Module):\n",
    "    def __init__(self,max_seq_len,embeddings,hidden_dim,n_layers,tagset_size,device=\"cuda\"):\n",
    "        super().__init__()\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.num_labels = tagset_size\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.embeddings = nn.Embedding.from_pretrained(embeddings,padding_idx=0)\n",
    "        self.lstm = nn.LSTM(input_size=embeddings.size()[1], hidden_size= self.hidden_dim , num_layers=n_layers)\n",
    "        self.hidden2tag = nn.Linear(self.hidden_dim,self.num_labels)\n",
    "        self.device = device\n",
    "        self.to(device)\n",
    "        \n",
    "    def forward(self,input_seq):\n",
    "        input_seq = input_seq.to(self.device)\n",
    "        embed_out =self.embeddings(input_seq)\n",
    "        lstm_out,_ = self.lstm(embed_out)\n",
    "        logits = self.hidden2tag(lstm_out)\n",
    "        return logits\n",
    "    \n",
    "    def evaluate(self,loader):\n",
    "        self.eval()\n",
    "        true_labels = []\n",
    "        pred_labels = []\n",
    "        for i, data in enumerate(loader):\n",
    "            x,y = data\n",
    "            logits = self.forward(x)\n",
    "            pred_label=torch.argmax(logits, dim=-1).cpu().numpy()\n",
    "            batch_size, _ = x.shape\n",
    "            for j in range(batch_size):\n",
    "                tags = y[j]\n",
    "                pred = pred_label[j]\n",
    "                for k in range(len(tags)):\n",
    "                    if tags[k] != 0:\n",
    "                        true_labels.append(tags[k])\n",
    "                        pred_labels.append(pred[k])\n",
    "        acc = accuracy(true_labels, pred_labels)  \n",
    "        return acc ,true_labels ,pred_labels          \n",
    "\n",
    "    def run_training(self,train_loader,dev_loader,epochs=100,learning_rate=5e-4,eval_every=5):\n",
    "        if str(self.device) == 'cpu':\n",
    "            print(\"Training only supported in GPU environment\")\n",
    "            return\n",
    "        torch.cuda.empty_cache()\n",
    "        self.to(self.device)\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=learning_rate)\n",
    "        loss_function = nn.CrossEntropyLoss(ignore_index=0)\n",
    "        for epoch in tqdm(range(epochs)):\n",
    "            self.train()\n",
    "            total_loss = 0\n",
    "            for i, data in enumerate(train_loader):\n",
    "                x,y=data\n",
    "                self.zero_grad()\n",
    "                logits = self.forward(x)\n",
    "                labels = torch.LongTensor(y).to(self.device)\n",
    "                loss = loss_function(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "                total_loss += loss\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            # print(\"Epoch {} | Loss: {}\".format(epoch, total_loss))\n",
    "            # if epoch % eval_every == eval_every-1:\n",
    "            #     acc,_,_ = self.evaluate(dev_loader)\n",
    "            #     print(\"Epoch {} | Accuracy: {}\".format(epoch, acc))\n",
    "        acc_train,_,_ = self.evaluate(train_loader)\n",
    "        acc_val,true_labels,pred_labels = self.evaluate(dev_loader)\n",
    "        print(\"# Model : Training Accuracy : {} Validation Accuracy: {} #\".format(acc_train,acc_val))  \n",
    "    \n",
    "    def predict(self,data):\n",
    "        x = torch.LongTensor(data)\n",
    "        self.eval()\n",
    "        predictions = []\n",
    "        logits = self.forward(x)\n",
    "        pred_label=torch.argmax(logits, dim=-1).cpu().numpy()\n",
    "        batch_size, _ = x.shape\n",
    "        for j in range(batch_size):\n",
    "            labels=[]\n",
    "            for k in range(len(x[j])):\n",
    "                if x[j][k] != 0:\n",
    "                    labels.append(pred_label[j][k])\n",
    "            predictions.append(labels)\n",
    "        return predictions\n",
    "\n",
    "    \n",
    "    def save(self,path):\n",
    "        torch.save(self.state_dict(), path)\n",
    "\n",
    "    def load(self,path):\n",
    "        self.load_state_dict(torch.load(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:34<00:00,  1.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Model : Training Accuracy : 0.9678347549069982 Validation Accuracy: 0.9634256472004816 #\n"
     ]
    }
   ],
   "source": [
    "set_seed(159)\n",
    "tagger = POSTagger(50,embeddings,128,2,len(tags))\n",
    "tagger.run_training(train_dataloader,dev_dataloader,50,0.0005,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagger.save(\"pos_tagger.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9668693009118541\n"
     ]
    }
   ],
   "source": [
    "acc,_,_=tagger.evaluate(test_dataloader)\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictor(model : POSTagger,sentence,tags):\n",
    "    tokens = sentence.split(\" \")\n",
    "    inv_map = {v: k for k, v in tags.items()}\n",
    "    vec = [sent_to_vector(sentence)]\n",
    "    predictions = model.predict(vec)\n",
    "    for i in range(len(tokens)):\n",
    "        print (tokens[i]+\"\\t\"+inv_map[predictions[0][i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'predictor' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/soumodiptab/workspaces/Intro-to-NLP/assignment-2/assignment-2.ipynb Cell 31\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu/home/soumodiptab/workspaces/Intro-to-NLP/assignment-2/assignment-2.ipynb#X42sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m predictor(tagger2,\u001b[39m\"\u001b[39m\u001b[39mMary had a little lamb\u001b[39m\u001b[39m\"\u001b[39m,tags)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'predictor' is not defined"
     ]
    }
   ],
   "source": [
    "predictor(tagger,\"Mary had a little lamb\",tags)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyper Parameter Tuning and model exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tag_analysis(data_loader,model: POSTagger,tags):\n",
    "    inv_map = {v: k for k, v in tags.items()}\n",
    "    acc,true_labels,pred_labels=model.evaluate(data_loader)\n",
    "    print(classification_report(true_labels,pred_labels,labels=list(tags.values()),target_names=list(tags.keys())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         PAD       0.00      0.00      0.00         0\n",
      "         ADJ       0.93      0.97      0.95       220\n",
      "         ADP       0.98      0.99      0.98      1434\n",
      "         ADV       0.95      0.72      0.82        76\n",
      "         AUX       0.96      1.00      0.98       256\n",
      "       CCONJ       1.00      1.00      1.00       109\n",
      "         DET       0.98      0.87      0.92       512\n",
      "        INTJ       0.97      1.00      0.99        36\n",
      "        NOUN       0.99      0.97      0.98      1166\n",
      "         NUM       0.95      0.96      0.95       127\n",
      "        PART       0.72      0.52      0.60        56\n",
      "        PRON       0.85      0.98      0.91       392\n",
      "       PROPN       0.98      0.99      0.99      1567\n",
      "         SYM       0.00      0.00      0.00         0\n",
      "        VERB       0.98      0.97      0.98       629\n",
      "\n",
      "   micro avg       0.97      0.97      0.97      6580\n",
      "   macro avg       0.82      0.80      0.80      6580\n",
      "weighted avg       0.97      0.97      0.97      6580\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/soumodiptab/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/soumodiptab/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/soumodiptab/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/soumodiptab/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/soumodiptab/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/soumodiptab/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "tag_analysis(test_dataloader,tagger,tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImprovedPOSTagger(POSTagger):\n",
    "    def __init__(self,max_seq_len,embeddings,hidden_dim,n_layers,tagset_size,dropout=0.8,bidirectional=True,device=\"cuda\"):\n",
    "        super().__init__(max_seq_len,embeddings,hidden_dim,n_layers,tagset_size,device)\n",
    "        self.lstm = nn.LSTM(input_size=embeddings.size()[1], hidden_size=hidden_dim, dropout=dropout, num_layers=n_layers, bidirectional=bidirectional)\n",
    "        self.hidden2tag = nn.Linear(self.hidden_dim*2,self.num_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ImprovedPOSTagger' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/soumodiptab/workspaces/Intro-to-NLP/assignment-2/assignment-2.ipynb Cell 37\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu/home/soumodiptab/workspaces/Intro-to-NLP/assignment-2/assignment-2.ipynb#X51sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m tagger2 \u001b[39m=\u001b[39m ImprovedPOSTagger(\u001b[39m50\u001b[39m,embeddings,\u001b[39m128\u001b[39m,\u001b[39m2\u001b[39m,\u001b[39mlen\u001b[39m(tags),\u001b[39m0.3\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/soumodiptab/workspaces/Intro-to-NLP/assignment-2/assignment-2.ipynb#X51sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m tagger2\u001b[39m.\u001b[39mrun_training(train_dataloader,dev_dataloader,\u001b[39m100\u001b[39m,\u001b[39m0.0005\u001b[39m,\u001b[39m5\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ImprovedPOSTagger' is not defined"
     ]
    }
   ],
   "source": [
    "tagger2 = ImprovedPOSTagger(50,embeddings,128,2,len(tags),0.3)\n",
    "tagger2.run_training(train_dataloader,dev_dataloader,100,0.0005,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagger2.save('pos_tagger2.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy for improved model: 0.9682370820668693\n"
     ]
    }
   ],
   "source": [
    "acc,_,_=tagger2.evaluate(test_dataloader)\n",
    "print(\"Test accuracy for improved model: \"+str(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyzer(train_dataloader,dev_dataloader,test_dataloader,tags):\n",
    "    lrs = [0.0005]\n",
    "    layers = [2]\n",
    "    hidden_dims = [128]\n",
    "    dropouts = [0.2,0.5,0.8]\n",
    "    best_accuracy = 0\n",
    "    best_config = {\"lr\":0,\"layers\":0,\"hidden_dim\":0,\"dropout\":0}\n",
    "    for lr in lrs:\n",
    "        for layer in layers:\n",
    "            for hidden_dim in hidden_dims:\n",
    "                for dropout in dropouts:\n",
    "                    set_seed(159)\n",
    "                    tagger = ImprovedPOSTagger(50,embeddings,hidden_dim,layer,len(tags),dropout=dropout)\n",
    "                    tagger.run_training(train_dataloader,dev_dataloader,50,lr,5)\n",
    "                    acc,true_labels,pred_labels=tagger.evaluate(test_dataloader)\n",
    "                    #score = f1_score(true_labels,pred_labels,len(tags))\n",
    "                    #cf = confusion_matrix(true_labels,pred_labels,len(tags))\n",
    "                    print(\"-------------------------------------------------------------------------------------\")\n",
    "                    print(\"# Model Parameters | Learning Rate : {} Layers : {} Hidden Dim : {} #\".format(lr,layer,hidden_dim))\n",
    "                    print(\"# Analysis | Test Accuracy : {} #\".format(acc))\n",
    "                    # print(\"# Confusion Matrix : # : \")\n",
    "                    # print(cf)\n",
    "                    print(\"-------------------------------------------------------------------------------------\")\n",
    "                    #tag_analysis(test_dataloader,tagger,tags)\n",
    "                    if acc > best_accuracy:\n",
    "                        tagger.save(\"pos_tagger.pt\")\n",
    "                        best_accuracy = acc\n",
    "                        best_config = {\"lr\":lr,\"layers\":layer,\"hidden_dim\":hidden_dim}\n",
    "                \n",
    "    print(\"Best Accuracy : {} Best Config : {}\".format(best_accuracy,best_config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:50<00:00,  1.01s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Model : Training Accuracy : 0.9697461720275409 Validation Accuracy: 0.9629741119807345 #\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/50 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------------------------------\n",
      "# Model Parameters | Learning Rate : 0.0005 Layers : 2 Hidden Dim : 128 #\n",
      "# Analysis | Test Accuracy : 0.967629179331307 #\n",
      "-------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:48<00:00,  1.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Model : Training Accuracy : 0.969705066283013 Validation Accuracy: 0.9641782059000602 #\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/50 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------------------------------\n",
      "# Model Parameters | Learning Rate : 0.0005 Layers : 2 Hidden Dim : 128 #\n",
      "# Analysis | Test Accuracy : 0.967629179331307 #\n",
      "-------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:55<00:00,  1.12s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Model : Training Accuracy : 0.9686157640530264 Validation Accuracy: 0.963275135460566 #\n",
      "-------------------------------------------------------------------------------------\n",
      "# Model Parameters | Learning Rate : 0.0005 Layers : 2 Hidden Dim : 128 #\n",
      "# Analysis | Test Accuracy : 0.9665653495440729 #\n",
      "-------------------------------------------------------------------------------------\n",
      "Best Accuracy : 0.967629179331307 Best Config : {'lr': 0.0005, 'layers': 2, 'hidden_dim': 128}\n"
     ]
    }
   ],
   "source": [
    "analyzer(train_dataloader,dev_dataloader,test_dataloader,tags)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
