{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "AK2EQbXLm3xC"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "import math\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FP6XmNWgm_ZG",
        "outputId": "1bc78c2e-f787-44ad-ae3d-81a176be3f79"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package treebank to\n",
            "[nltk_data]     /home/soumodiptab/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/treebank.zip.\n",
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     /home/soumodiptab/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     /home/soumodiptab/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: datasets in /home/soumodiptab/.local/lib/python3.8/site-packages (2.10.1)\n",
            "Requirement already satisfied: responses<0.19 in /home/soumodiptab/.local/lib/python3.8/site-packages (from datasets) (0.18.0)\n",
            "Requirement already satisfied: multiprocess in /home/soumodiptab/.local/lib/python3.8/site-packages (from datasets) (0.70.14)\n",
            "Requirement already satisfied: numpy>=1.17 in /home/soumodiptab/.local/lib/python3.8/site-packages (from datasets) (1.22.1)\n",
            "Requirement already satisfied: xxhash in /home/soumodiptab/.local/lib/python3.8/site-packages (from datasets) (3.2.0)\n",
            "Requirement already satisfied: dill<0.3.7,>=0.3.0 in /home/soumodiptab/.local/lib/python3.8/site-packages (from datasets) (0.3.6)\n",
            "Requirement already satisfied: pandas in /home/soumodiptab/.local/lib/python3.8/site-packages (from datasets) (1.3.5)\n",
            "Requirement already satisfied: fsspec[http]>=2021.11.1 in /home/soumodiptab/.local/lib/python3.8/site-packages (from datasets) (2023.3.0)\n",
            "Requirement already satisfied: packaging in /home/soumodiptab/.local/lib/python3.8/site-packages (from datasets) (21.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /home/soumodiptab/.local/lib/python3.8/site-packages (from datasets) (2.27.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0.0,>=0.2.0 in /home/soumodiptab/.local/lib/python3.8/site-packages (from datasets) (0.5.1)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /home/soumodiptab/.local/lib/python3.8/site-packages (from datasets) (4.65.0)\n",
            "Requirement already satisfied: aiohttp in /home/soumodiptab/.local/lib/python3.8/site-packages (from datasets) (3.8.4)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/lib/python3/dist-packages (from datasets) (5.3.1)\n",
            "Requirement already satisfied: pyarrow>=6.0.0 in /home/soumodiptab/.local/lib/python3.8/site-packages (from datasets) (11.0.0)\n",
            "Requirement already satisfied: urllib3>=1.25.10 in /home/soumodiptab/.local/lib/python3.8/site-packages (from responses<0.19->datasets) (1.26.9)\n",
            "Requirement already satisfied: pytz>=2017.3 in /home/soumodiptab/.local/lib/python3.8/site-packages (from pandas->datasets) (2021.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /home/soumodiptab/.local/lib/python3.8/site-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/soumodiptab/.local/lib/python3.8/site-packages (from packaging->datasets) (3.0.6)\n",
            "Requirement already satisfied: idna<4,>=2.5; python_version >= \"3\" in /usr/lib/python3/dist-packages (from requests>=2.19.0->datasets) (2.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests>=2.19.0->datasets) (2019.11.28)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0; python_version >= \"3\" in /home/soumodiptab/.local/lib/python3.8/site-packages (from requests>=2.19.0->datasets) (2.0.12)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/soumodiptab/.local/lib/python3.8/site-packages (from huggingface-hub<1.0.0,>=0.2.0->datasets) (4.0.1)\n",
            "Requirement already satisfied: filelock in /home/soumodiptab/.local/lib/python3.8/site-packages (from huggingface-hub<1.0.0,>=0.2.0->datasets) (3.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /home/soumodiptab/.local/lib/python3.8/site-packages (from aiohttp->datasets) (6.0.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /home/soumodiptab/.local/lib/python3.8/site-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /home/soumodiptab/.local/lib/python3.8/site-packages (from aiohttp->datasets) (4.0.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /home/soumodiptab/.local/lib/python3.8/site-packages (from aiohttp->datasets) (1.3.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/lib/python3/dist-packages (from aiohttp->datasets) (19.3.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /home/soumodiptab/.local/lib/python3.8/site-packages (from aiohttp->datasets) (1.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.14.0)\n"
          ]
        }
      ],
      "source": [
        "nltk.download('treebank')\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "!pip install datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ExbnBcoBmKxY",
        "outputId": "ac6579a8-856e-4690-b9af-9d50fd5616c5"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     /home/soumodiptab/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 183
        },
        "id": "MbIRLwmyx3md",
        "outputId": "74a6a872-7553-47ad-ad29-ad38f59d133f"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'torch' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[1;32m/home/soumodiptab/workspaces/Intro-to-NLP/assignment-4/NLP4.ipynb Cell 4\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu/home/soumodiptab/workspaces/Intro-to-NLP/assignment-4/NLP4.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m device \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mdevice(\u001b[39m'\u001b[39m\u001b[39mcuda\u001b[39m\u001b[39m'\u001b[39m \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mis_available() \u001b[39melse\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/soumodiptab/workspaces/Intro-to-NLP/assignment-4/NLP4.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m torch\u001b[39m.\u001b[39mmanual_seed(\u001b[39m0\u001b[39m)\n",
            "\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"
          ]
        }
      ],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "rj5An3dhoEDR"
      },
      "outputs": [],
      "source": [
        "from nltk.corpus import treebank"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "ain346jnlce3"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "No config specified, defaulting to: sst/default\n",
            "Found cached dataset sst (/home/soumodiptab/.cache/huggingface/datasets/sst/default/1.0.0/b8a7889ef01c5d3ae8c379b84cc4080f8aad3ac2bc538701cbe0ac6416fb76ff)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8239f19e878441a183fd55055d06fd15",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/3 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "dataset = load_dataset('sst')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "QXWxgeRAmWNF"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['sentence', 'label', 'tokens', 'tree'],\n",
            "        num_rows: 8544\n",
            "    })\n",
            "    validation: Dataset({\n",
            "        features: ['sentence', 'label', 'tokens', 'tree'],\n",
            "        num_rows: 1101\n",
            "    })\n",
            "    test: Dataset({\n",
            "        features: ['sentence', 'label', 'tokens', 'tree'],\n",
            "        num_rows: 2210\n",
            "    })\n",
            "})\n"
          ]
        }
      ],
      "source": [
        "print(dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "WtOgIgT4n48u"
      },
      "outputs": [],
      "source": [
        "train_sentences_raw = [nltk.word_tokenize(sentence) for sentence in dataset['train'][:]['sentence']]\n",
        "train_labels = [label for label in dataset['train'][:]['label']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "qJk4GgpGpLc8"
      },
      "outputs": [],
      "source": [
        "val_sentences_raw = [nltk.word_tokenize(sentence) for sentence in dataset['validation'][:]['sentence']]\n",
        "val_labels = [label for label in dataset['validation'][:]['label']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "5Aoa8T-XpM_s"
      },
      "outputs": [],
      "source": [
        "test_sentences_raw = [nltk.word_tokenize(sentence) for sentence in dataset['test'][:]['sentence']]\n",
        "test_labels = [label for label in dataset['test'][:]['label']]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g3LxQlVLpaJs"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "LE1bbtLBoJfS"
      },
      "outputs": [],
      "source": [
        "stop_words = set(stopwords.words('english'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "l1HoUSR2o6xC"
      },
      "outputs": [],
      "source": [
        "def remove_stopwords(sentences):\n",
        "    filtered_sentences = []\n",
        "\n",
        "    for sentence in sentences:\n",
        "        filtered_sentence = [word for word in sentence if word.lower() not in stop_words]\n",
        "        filtered_sentences.append(filtered_sentence)\n",
        "\n",
        "    return filtered_sentences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "k2ybf_xIqB9H"
      },
      "outputs": [],
      "source": [
        "stemmer = PorterStemmer()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "IuzzuDqypHxc"
      },
      "outputs": [],
      "source": [
        "def stem(filtered_sentences):\n",
        "    stemmed_sentences = []\n",
        "\n",
        "    for sentence in filtered_sentences:\n",
        "        stemmed_sentence = [stemmer.stem(word.lower()) for word in sentence]\n",
        "        stemmed_sentences.append(stemmed_sentence)\n",
        "\n",
        "    return stemmed_sentences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "OICJSLCjxL4G"
      },
      "outputs": [],
      "source": [
        "def pre_process(sentences):\n",
        "    fil_sentances = remove_stopwords(sentences)\n",
        "    stemmed_sentences = stem(fil_sentances)\n",
        "    return stemmed_sentences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "jOCYJH7ppxJ6"
      },
      "outputs": [],
      "source": [
        "train_sentences = pre_process(train_sentences_raw)\n",
        "val_sentences = pre_process(val_sentences_raw)\n",
        "test_sentences = pre_process(test_sentences_raw)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "iaCf20IzwvOq"
      },
      "outputs": [],
      "source": [
        "def get_max_len(sentences):\n",
        "    maxx = 0\n",
        "    for sentance in sentences:\n",
        "        maxx = max(len(sentance),maxx)\n",
        "    return maxx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "tb6jqkwowvTg"
      },
      "outputs": [],
      "source": [
        "max_sentence_size = get_max_len(train_sentences)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "5Wq0JK--yNxX"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "38"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "max_sentence_size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "d7vLpWDhwucp"
      },
      "outputs": [],
      "source": [
        "def add_padding_normalize(list_list_words,max_sentence_size):\n",
        "    for list_words in list_list_words:\n",
        "        diff = max_sentence_size - len(list_words)\n",
        "        if diff > 0:\n",
        "            padding = [ \"<pad>\" for i in range(diff)]\n",
        "            list_words.extend(padding)\n",
        "        else:\n",
        "            list_words[:] = list_words[:max_sentence_size]\n",
        "    return list_list_words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "u30iY2-tsjs7"
      },
      "outputs": [],
      "source": [
        "train_sentences = add_padding_normalize(train_sentences,max_sentence_size)\n",
        "val_sentences = add_padding_normalize(val_sentences,max_sentence_size)\n",
        "test_sentences = add_padding_normalize(test_sentences,max_sentence_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "eWi3wseOxydU"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "8544\n",
            "1101\n",
            "2210\n"
          ]
        }
      ],
      "source": [
        "print(len(train_sentences))\n",
        "print(len(val_sentences))\n",
        "print(len(test_sentences))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "9oXUc5nf2VP4"
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'google.colab'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[1;32m/home/soumodiptab/workspaces/Intro-to-NLP/assignment-4/NLP4.ipynb Cell 24\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu/home/soumodiptab/workspaces/Intro-to-NLP/assignment-4/NLP4.ipynb#X32sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mgoogle\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcolab\u001b[39;00m \u001b[39mimport\u001b[39;00m drive\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/soumodiptab/workspaces/Intro-to-NLP/assignment-4/NLP4.ipynb#X32sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m drive\u001b[39m.\u001b[39mmount(\u001b[39m'\u001b[39m\u001b[39m/content/gdrive\u001b[39m\u001b[39m'\u001b[39m)\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'google.colab'"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "_-zY8OtTCa3F"
      },
      "outputs": [],
      "source": [
        "import torchtext\n",
        "import torch\n",
        "import torchtext.vocab as vocab\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "UpX-BMktCSaL"
      },
      "outputs": [],
      "source": [
        "vocab = torchtext.vocab.build_vocab_from_iterator(train_sentences, min_freq=1,specials=[\"<unk>\"]) \n",
        "vocab.set_default_index(vocab[\"<unk>\"])       "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "OQzmJrOIE-AV"
      },
      "outputs": [],
      "source": [
        "def read_emb(path):\n",
        "    with open(path, encoding=\"utf-8\") as file:\n",
        "        word_embedding_dim = len(file.readline().split(\" \")) - 1\n",
        "    embeddings = np.zeros((len(vocab), word_embedding_dim))\n",
        "    last_idx = 0\n",
        "\n",
        "    with open(path, 'r', encoding='utf-8') as f:\n",
        "        for i, line in enumerate(f):\n",
        "            word,*vector  = line.split()\n",
        "            if word in vocab:\n",
        "                  indx = vocab[word]\n",
        "                  embeddings[indx] = vector\n",
        "        return torch.FloatTensor(embeddings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "WNEY3vhgEQ2c"
      },
      "outputs": [],
      "source": [
        "embeddings = read_emb('embeddings/glove/glove.6B.100d.txt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "QpX9sHxEshSZ"
      },
      "outputs": [],
      "source": [
        "word_to_idx = vocab.get_stoi()\n",
        "idx_to_word = [word for word, idx in sorted(word_to_idx.items(), key=lambda x: x[1])]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "jOXhjh9Drfog"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "12059"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(vocab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "QK_WQzAVrj6d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "12059"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(word_to_idx)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "fDXO6dlR1rhq"
      },
      "outputs": [],
      "source": [
        "def vectorize_tokens(list_words):\n",
        "    return [vocab[word] for word in list_words] "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "slg6HhhH1ajq"
      },
      "outputs": [],
      "source": [
        "def vectorize_dataset(list_list_words):\n",
        "    x = []\n",
        "    for list_words in list_list_words:\n",
        "        words = vectorize_tokens(list_words)\n",
        "        x.append(words)  \n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "_5ZDMW_M1iUn"
      },
      "outputs": [],
      "source": [
        "train_vector = vectorize_dataset(train_sentences)\n",
        "val_vector = vectorize_dataset(val_sentences)\n",
        "test_vector = vectorize_dataset(test_sentences)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "38"
            ]
          },
          "execution_count": 38,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(train_vector[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [
        {
          "ename": "AttributeError",
          "evalue": "'list' object has no attribute 'shape'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[1;32m/home/soumodiptab/workspaces/Intro-to-NLP/assignment-4/NLP4.ipynb Cell 35\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu/home/soumodiptab/workspaces/Intro-to-NLP/assignment-4/NLP4.ipynb#Y101sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m train_vector\u001b[39m.\u001b[39;49mshape\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'shape'"
          ]
        }
      ],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "5to4OuHR-Fev"
      },
      "outputs": [],
      "source": [
        "\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "N7Eaug3isyIM"
      },
      "outputs": [],
      "source": [
        "class ELMoDataset(Dataset):\n",
        "    def __init__(self, data):\n",
        "        self.data = data\n",
        "        self.backward_context = [sentance[:-1] for sentance in self.data]\n",
        "        self.forward_context = [sentance[1:] for sentance in self.data]\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        return self.backward_context[idx], self.forward_context[idx]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "EzfRwYUrs2gA"
      },
      "outputs": [],
      "source": [
        "train_dataset = ELMoDataset(torch.tensor(train_vector))\n",
        "test_dataset = ELMoDataset(torch.tensor(test_vector))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "wYwpc65as4ET"
      },
      "outputs": [],
      "source": [
        "batch_size = 32"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "sJZJoQ4vs3Yo"
      },
      "outputs": [],
      "source": [
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "7SLZn4XV6bJQ"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "100"
            ]
          },
          "execution_count": 44,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "embeddings.size()[1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "rsE9-tq0Bry3"
      },
      "outputs": [],
      "source": [
        "class ELMo(nn.Module):\n",
        "    def __init__(self, vocab_size, embeddings, hidden_dim):\n",
        "        super(ELMo, self).__init__()\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embedding_dim =  embeddings.size()[1]\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.embedding = nn.Embedding.from_pretrained(embeddings)\n",
        "        self.embedding.weight = nn.Parameter(self.embedding.weight, requires_grad=True)\n",
        "        self.lstm1 = nn.LSTM(self.embedding_dim, hidden_dim, batch_first=True, bidirectional=True)\n",
        "        self.lstm2 = nn.LSTM(hidden_dim*2, hidden_dim, batch_first=True, bidirectional=True)\n",
        "        self.linear1 = nn.Linear(self.embedding_dim, hidden_dim)\n",
        "        self.linear_out = nn.Linear(hidden_dim*2, vocab_size)\n",
        "\n",
        "    def forward(self,back_data):\n",
        "        back_embed = self.embedding(back_data)\n",
        "        back_lstm1, _ = self.lstm1(back_embed)   #output : batch_size x max_len x hidden_dim*2\n",
        "        back_lstm2, _ = self.lstm2(back_lstm1)\n",
        "        linear_out = self.linear_out(back_lstm2)\n",
        "        return linear_out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "rMyLNv7YOX97"
      },
      "outputs": [],
      "source": [
        "hidden_dim = 256\n",
        "num_layers = 2\n",
        "vocab_size = len(word_to_idx)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7f024c7f0bb0>"
            ]
          },
          "execution_count": 48,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "torch.manual_seed(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "1KeLpQeABhIa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ELMo(\n",
            "  (embedding): Embedding(12059, 100)\n",
            "  (lstm1): LSTM(100, 100, batch_first=True, bidirectional=True)\n",
            "  (lstm2): LSTM(200, 100, batch_first=True, bidirectional=True)\n",
            "  (linear1): Linear(in_features=100, out_features=100, bias=True)\n",
            "  (linear_out): Linear(in_features=200, out_features=12059, bias=True)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "elmo = ELMo(vocab_size, embeddings, 100)\n",
        "elmo.to(device)\n",
        "print(elmo)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "bGMCxPwPOzRy"
      },
      "outputs": [],
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(elmo.parameters(), lr=0.001)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "zKYnKr_NwvmG"
      },
      "outputs": [],
      "source": [
        "elmo_losses = {'train': [], 'val': [], 'epoch' : []}\n",
        "epochs = 4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "5X-VtjADOsKP"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 1/4 Step: 0 Loss: 9.3722505569458 Train Loss: 9.3722505569458\n"
          ]
        }
      ],
      "source": [
        "for epoch in range(epochs):\n",
        "    train_loss = 0\n",
        "    elmo.train()\n",
        "    for i, (forward_context, backward_context) in enumerate(train_loader):\n",
        "        forward_context, backward_context = forward_context.to(device), backward_context.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = elmo(backward_context)\n",
        "        output = output.view(-1, len(vocab))\n",
        "        target = forward_context.view(-1)\n",
        "        loss = criterion(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        train_loss += loss.item()\n",
        "        if i%500 == 0:\n",
        "            print('Epoch: {}/{}'.format(epoch+1, epochs), 'Step: {}'.format(i), 'Loss: {}'.format(loss.item()), 'Train Loss: {}'.format(train_loss/(i+1)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e1-EvRGl-My_"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AjCjnoTMFucb"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_k2ntAwxFxcL"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "noLRN-Oj2Ulu"
      },
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
