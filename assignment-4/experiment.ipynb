{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset,load_from_disk\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No config specified, defaulting to: sst/default\n",
      "Found cached dataset sst (/home/soumodiptab/.cache/huggingface/datasets/sst/default/1.0.0/b8a7889ef01c5d3ae8c379b84cc4080f8aad3ac2bc538701cbe0ac6416fb76ff)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6dc6e2fa3d674f61ac7ef7d8db4b87ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = load_dataset('sst')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b21766ab312f44128de71521a39093ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/5.14k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e45cf425b6da4e1fb37ecf17d4eecaf0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading metadata:   0%|          | 0.00/2.88k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c20ed8f5537a4181a1033867786cd416",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/8.67k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset multi_nli/default to /home/soumodiptab/.cache/huggingface/datasets/multi_nli/default/0.0.0/591f72eb6263d1ab527561777936b199b714cda156d35716881158a2bd144f39...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b9e270366324286a2e4899767d3dce8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/227M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb3991d7f8cd4dfc835174029983071f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/392702 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2caf4f7be38f44b1a4d8a53e6912d125",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation_matched split:   0%|          | 0/9815 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db726ce2910e424da85a8d17005a261a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation_mismatched split:   0%|          | 0/9832 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset multi_nli downloaded and prepared to /home/soumodiptab/.cache/huggingface/datasets/multi_nli/default/0.0.0/591f72eb6263d1ab527561777936b199b714cda156d35716881158a2bd144f39. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f24f0453f464437d8ee76f8d395327d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset2 =load_dataset('multi_nli')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2a51d4ff37148d48fd8cf923003c5e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/392702 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c15604eff5d45eaae88e689aa4319fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/9815 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c6ad567578641edad4f45fef11a3f5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/9832 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset2.save_to_disk('multi_nli.hf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset2 = load_from_disk('multi_nli.hf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_from_disk('./data/sst.hf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = dataset['train']\n",
    "dataset_test = dataset['test']\n",
    "dataset_val = dataset['validation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchtext\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "import re\n",
    "from cleantext import clean\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataCleaner:\n",
    "    def __init__(self):\n",
    "        self.tokenizer = get_tokenizer('basic_english')\n",
    "        self.nlp = spacy.load('en_core_web_sm', disable = ['ner', 'tagger', 'parser'])\n",
    "        self.stopwords = self.nlp.Defaults.stop_words\n",
    "\n",
    "    def replace_dates(self,text):\n",
    "        date_format_a = re.sub(r'\\d{1,2}/\\d{1,2}/\\d{2,4}', ' <DATE> ', text)\n",
    "        date_format_b = re.sub(\n",
    "            r'[A-Za-z]{2,8}\\s\\d{1,2},?\\s\\d {4}', ' <DATE> ', date_format_a)\n",
    "        date_format_c = re.sub(\n",
    "            r'\\d{2} [A-Z][a-z]{2,8} \\d{4}', ' <DATE> ', date_format_b)\n",
    "        return date_format_c\n",
    "\n",
    "    def replace_hash_tags(self,text):\n",
    "        return re.sub(r'(\\s|^)#(\\w+)', ' <HASHTAG> ', text)\n",
    "\n",
    "    def remove_special_characters(self,text):\n",
    "            # remove all special characters \n",
    "        return re.sub(r'[^A-Za-z0-9\\s]', ' ', text)\n",
    "\n",
    "    def remove_extra_spaces(self,text):\n",
    "        return re.sub(r'\\s{2,}', ' ', text)\n",
    "\n",
    "    def replace_hyphenated_words(self,text):\n",
    "        # replace hyphenated words with words seperated by space\n",
    "        return re.sub(r'(\\w+)-(\\w+)', r' \\1 \\2 ', text)\n",
    "\n",
    "    def clean_text(self,line):\n",
    "        # line = line.strip()\n",
    "        # line = re.sub(r'<|>', ' ', line)\n",
    "        # line = self.replace_dates(line)\n",
    "        # line = self.replace_hyphenated_words(line)\n",
    "        # line = self.replace_hash_tags(line)\n",
    "        # # remove < and > from the text\n",
    "        # line = clean(line, no_emoji=True,\n",
    "        #             no_urls=True,\n",
    "        #             no_emails=True,\n",
    "        #             no_phone_numbers=True,\n",
    "        #             no_currency_symbols=True,           \n",
    "        #             replace_with_url=\" <URL> \",\n",
    "        #             replace_with_email=\" <EMAIL> \",\n",
    "        #             replace_with_phone_number=\" <PHONE> \",\n",
    "        #             replace_with_currency_symbol=\" <CURRENCY> \",\n",
    "        #             lower=True)\n",
    "        line = self.remove_special_characters(line)\n",
    "        line = clean(line,no_numbers=True,\n",
    "                     no_digits=True,\n",
    "                     no_punct=True,\n",
    "                     replace_with_number=\" <NUMBER> \",\n",
    "                     replace_with_digit=\" \",\n",
    "                     replace_with_punct=\"\",\n",
    "                     lower=True)\n",
    "        line = self.remove_extra_spaces(line)\n",
    "        tokens=self.tokenizer(line)\n",
    "        return \" \".join(tokens)\n",
    "    \n",
    "    def remove_stopwords(self,text):\n",
    "        tokens = self.tokenizer(text)\n",
    "        return \" \".join([token for token in tokens if token not in self.stopwords])\n",
    "    \n",
    "    def lemmatize(self,text):\n",
    "        doc = self.nlp(text)\n",
    "        return \" \".join([token.lemma_ for token in doc])\n",
    "    \n",
    "    def process(self,text):\n",
    "        text = self.clean_text(text)\n",
    "        text = self.remove_stopwords(text)\n",
    "        text = self.lemmatize(text)\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "datacleaner = DataCleaner()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52\n"
     ]
    }
   ],
   "source": [
    "size = 0\n",
    "for text in dataset_train:\n",
    "    p = text['sentence'].split()\n",
    "    size=max(len(p),size)\n",
    "print(size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Rock is destined to be the 21st Century 's new `` Conan '' and that he 's going to make a splash even greater than Arnold Schwarzenegger , Jean-Claud Van Damme or Steven Segal .\n",
      "rock destined st century s new conan s going splash greater arnold schwarzenegger jean claud van damme steven segal\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/soumodiptab/.local/lib/python3.8/site-packages/spacy/pipeline/lemmatizer.py:211: UserWarning: [W108] The rule-based lemmatizer did not find POS annotation for one or more tokens. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "  warnings.warn(Warnings.W108)\n"
     ]
    }
   ],
   "source": [
    "print(dataset_train[0][\"sentence\"])\n",
    "print(datacleaner.process(dataset_train[0][\"sentence\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "from  tqdm import tqdm\n",
    "from collections import Counter\n",
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataPipeline(Dataset):\n",
    "    def __init__(self, filename,type,max_seq_len=50,min_freq=3,vocab=None):\n",
    "        self.data,self.target = self.read_data(filename,type)\n",
    "        self.max_seq_len = max_seq_len\n",
    "        if vocab is None:\n",
    "            self.vocab, self.ind2vocab,self.word_count = self.build_vocab(self.data,min_freq)\n",
    "        else:\n",
    "            self.vocab = vocab\n",
    "            self.ind2vocab = {v: k for k, v in vocab.items()}\n",
    "            # self.word_count = self.get_word_count(vocab,self.data)\n",
    "        self.ind2vocab = {ind: word for word, ind in self.vocab.items()}\n",
    "        \n",
    "\n",
    "    def get_vocab(self):\n",
    "        return self.vocab\n",
    "    def word_to_ind(self,word):\n",
    "        if word in self.vocab:\n",
    "            return self.vocab[word]\n",
    "        else:\n",
    "            return 1\n",
    "    def ind_to_word(self,ind):\n",
    "        return self.ind2vocab[ind]\n",
    "    \n",
    "    def read_data(self,filename,type):\n",
    "        datacleaner = DataCleaner()\n",
    "        data =load_from_disk(filename)\n",
    "        processed_data = []\n",
    "        target = []\n",
    "        for line in tqdm(data[type]):\n",
    "            processed_data.append(datacleaner.process(line['sentence']).split(\" \"))\n",
    "            target.append(line['label'])\n",
    "        return processed_data,target\n",
    "    \n",
    "    def get_word_count(self,vocab,data):\n",
    "        word_count = {0: 0}\n",
    "        for line in data:\n",
    "            for word in line:\n",
    "                if word in vocab:\n",
    "                    word_count[vocab[word]] += 1\n",
    "                else:\n",
    "                    word_count[0] += 1\n",
    "        return word_count\n",
    "    \n",
    "    def most_common(self,n):\n",
    "        counter = Counter(self.word_count)\n",
    "        common = counter.most_common(n)\n",
    "        ind_freq = dict(common)\n",
    "        # convert to word frequency\n",
    "        word_freq = {}\n",
    "        for ind in ind_freq:\n",
    "            word_freq[self.ind2vocab[ind]] = ind_freq[ind]\n",
    "        return word_freq\n",
    "    \n",
    "    @staticmethod\n",
    "    def build_vocab(data,min_freq):\n",
    "        word_set = {}\n",
    "        print('Building vocab:')\n",
    "        for line in tqdm(data):\n",
    "            for word in line:\n",
    "                if word not in word_set:\n",
    "                    word_set[word]=1\n",
    "                else:\n",
    "                    word_set[word]+=1\n",
    "        # sort the vocab\n",
    "        word_list = sorted(list(word_set))\n",
    "        word_count = {0: 0, 1: 0}\n",
    "        vocab_dict = {\"<pad>\":0,\"<unk>\": 1}\n",
    "        i=2\n",
    "        for word in tqdm(word_list):\n",
    "            if word_set[word] >= min_freq:\n",
    "                vocab_dict[word] = i\n",
    "                word_count[i] = word_set[word]\n",
    "                i+=1\n",
    "            else:\n",
    "                word_count[0] += word_set[word]\n",
    "        ind2word = {v: k for k, v in vocab_dict.items()}\n",
    "        print('Vocab size: {}'.format(len(vocab_dict)))\n",
    "        return vocab_dict, ind2word, word_count\n",
    "\n",
    "    def total_count(self):\n",
    "        return sum(self.word_count.values())\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sent = self.data[idx]\n",
    "        label = self.target[idx]\n",
    "        # paddding the sentences to create sequences of same length\n",
    "        if len(sent) < self.max_seq_len:\n",
    "            sent=[self.word_to_ind(token) for token in sent]+[self.word_to_ind(\"<pad>\") for _ in range(self.max_seq_len - len(sent))]\n",
    "        return torch.LongTensor(sent),torch.Tensor([label])\n",
    "    \n",
    "    def get_batches(self, batch_size):\n",
    "        return DataLoader(self, batch_size=batch_size, shuffle=False,drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ElmoDataset(DataPipeline):\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    def __getitem__(self, idx):\n",
    "        sent = self.data[idx]\n",
    "        label = self.target[idx]\n",
    "        # paddding the sentences to create sequences of same length\n",
    "        if len(sent) < self.max_seq_len:\n",
    "            sent=[self.word_to_ind(token) for token in sent]+[self.word_to_ind(\"<pad>\") for _ in range(self.max_seq_len - len(sent))]\n",
    "        forward_data = sent[1:]\n",
    "        backward_data = sent[:-1]\n",
    "        return torch.LongTensor(forward_data),torch.LongTensor(backward_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8544/8544 [00:21<00:00, 389.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building vocab:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8544/8544 [00:00<00:00, 356624.57it/s]\n",
      "100%|██████████| 14886/14886 [00:00<00:00, 677787.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 14888\n"
     ]
    }
   ],
   "source": [
    "data_train = ElmoDataset('data/sst.hf','train',80,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1101/1101 [00:03<00:00, 359.17it/s]\n"
     ]
    }
   ],
   "source": [
    "data_validation = ElmoDataset('data/sst.hf','validation',80,1,data_train.get_vocab())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'s': 2612,\n",
       " 'film': 1166,\n",
       " 'movie': 1016,\n",
       " 't': 705,\n",
       " 'n': 695,\n",
       " 'like': 516,\n",
       " 'number': 408,\n",
       " '<': 391,\n",
       " '>': 391,\n",
       " 'story': 355}"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_embeddings(vocab,embeddings_file,dimension):\n",
    "    # load only the embeddings that are in the vocab\n",
    "    embeddings = np.zeros((len(vocab), dimension))\n",
    "    with open(embeddings_file, 'r') as f:\n",
    "        for line in tqdm(f):\n",
    "            line = line.split()\n",
    "            word = line[0]\n",
    "            if word in vocab:\n",
    "                embeddings[vocab[word]] = np.array(line[1:], dtype=np.float32)\n",
    "    return torch.Tensor(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "400000it [00:04, 98159.26it/s] \n"
     ]
    }
   ],
   "source": [
    "embedding_matix=load_embeddings(data_train.get_vocab(),'embeddings/glove/glove.6B.100d.txt',100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ELMo(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, max_len, embedding_matrix):\n",
    "        super(ELMo, self).__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.max_len = max_len\n",
    "        self.embedding = nn.Embedding.from_pretrained(embedding_matrix)\n",
    "        self.embedding.weight = nn.Parameter(self.embedding.weight, requires_grad=True)\n",
    "        self.lstm1 = nn.LSTM(embedding_dim, hidden_dim, batch_first=True, bidirectional=True)\n",
    "        self.lstm2 = nn.LSTM(hidden_dim*2, hidden_dim, batch_first=True, bidirectional=True)\n",
    "        self.linear_out = nn.Linear(hidden_dim*2, vocab_size)\n",
    "    def forward(self,back_data):\n",
    "        back_embed = self.embedding(back_data)\n",
    "        back_lstm1, _ = self.lstm1(back_embed)\n",
    "        #back_lstm1 is shape of (batch_size, max_len, hidden_dim*2)\n",
    "        back_lstm2, _ = self.lstm2(back_lstm1)\n",
    "        linear_out = self.linear_out(back_lstm2)\n",
    "        return linear_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "elmo = ELMo(len(data_train.get_vocab()),100,100,80,embedding_matix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ELMo(\n",
      "  (embedding): Embedding(14876, 100)\n",
      "  (lstm1): LSTM(100, 100, batch_first=True, bidirectional=True)\n",
      "  (lstm2): LSTM(200, 100, batch_first=True, bidirectional=True)\n",
      "  (linear1): Linear(in_features=100, out_features=100, bias=True)\n",
      "  (linear_out): Linear(in_features=200, out_features=14876, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(elmo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ElmoTrainer:\n",
    "    def __init__(self,epochs=20,lr=0.001,batch_size=50,print_every=1,device='cpu'):\n",
    "        self.epochs = epochs\n",
    "        self.lr = lr\n",
    "        self.batch_size = batch_size\n",
    "        self.print_every = print_every\n",
    "        self.device = device\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        self.lowest_validation_loss = float('inf')\n",
    "    \n",
    "    def train(self,model : ELMo,model_save_path,train_data,validation_data):\n",
    "        self.optimizer = optim.Adam(model.parameters(), lr=self.lr)\n",
    "        model.to(self.device)\n",
    "        for epoch in range(len(range(self.epochs))):\n",
    "            model.train()\n",
    "            train_loader = train_data.get_batches(self.batch_size)\n",
    "            training_loss = 0\n",
    "            for i,(forward_data,backward_data) in tqdm(enumerate(train_loader)):\n",
    "                forward_data = forward_data.to(self.device)\n",
    "                backward_data = backward_data.to(self.device)\n",
    "                self.optimizer.zero_grad()\n",
    "                output = model(backward_data)\n",
    "                output = output.view(-1, model.vocab_size)\n",
    "                target = forward_data.view(-1)\n",
    "                loss = self.criterion(output, target)\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                training_loss += loss.item()\n",
    "        if epoch % self.print_every == 0:\n",
    "            print('Training Loss : {}'.format(training_loss/len(train_loader)))\n",
    "        self.__validate(model,model_save_path,validation_data)\n",
    "\n",
    "    def __validate(self,model:ELMo,model_save_path,validation_data):\n",
    "        model.eval()\n",
    "        model.to(self.device)\n",
    "        validation_loader = validation_data.get_batches(self.batch_size)\n",
    "        validation_loss = 0\n",
    "        for i,(forward_data,backward_data) in tqdm(enumerate(validation_loader)):\n",
    "            forward_data = forward_data.to(self.device)\n",
    "            backward_data = backward_data.to(self.device)\n",
    "            output = model(backward_data)\n",
    "            output = output.view(-1, len(model.vocab_size))\n",
    "            target = forward_data.view(-1)\n",
    "            loss = self.criterion(output, target)\n",
    "            validation_loss += loss.item()\n",
    "        if validation_loss < self.lowest_validation_loss:\n",
    "            self.lowest_validation_loss = validation_loss\n",
    "            torch.save(model.state_dict(), model_save_path)\n",
    "        print('Validation Loss : {}'.format(validation_loss/len(validation_loader)))\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = ElmoTrainer(epochs=20,lr=0.001,batch_size=50,print_every=2,device='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "trainer.train(elmo,'model/elmo.pt',data_train,data_validation)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
